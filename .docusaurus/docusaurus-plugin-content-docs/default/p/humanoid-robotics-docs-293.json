{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","href":"/humanoid-robotics/docs/intro","label":"Physical AI & Humanoid Robotics: A Comprehensive Guide","docId":"intro","unlisted":false},{"type":"category","label":"Module 1: ROS 2 (Robotic Nervous System)","items":[{"type":"link","href":"/humanoid-robotics/docs/module1/ROS-2-Robotic-Nervous-System/","label":"Module 1: ROS 2 (Robotic Nervous System)","docId":"module1/ROS-2-Robotic-Nervous-System/index","unlisted":false},{"type":"link","href":"/humanoid-robotics/docs/module1/ROS-2-Robotic-Nervous-System/chapter1-nodes-topics-services","label":"Chapter 1: Nodes, Topics, and Services","docId":"module1/ROS-2-Robotic-Nervous-System/chapter1-nodes-topics-services","unlisted":false},{"type":"link","href":"/humanoid-robotics/docs/module1/ROS-2-Robotic-Nervous-System/chapter2-rclpy-integration","label":"Chapter 2: rclpy Integration","docId":"module1/ROS-2-Robotic-Nervous-System/chapter2-rclpy-integration","unlisted":false},{"type":"link","href":"/humanoid-robotics/docs/module1/ROS-2-Robotic-Nervous-System/chapter3-urdf-humanoids","label":"Chapter 3: URDF for Humanoids","docId":"module1/ROS-2-Robotic-Nervous-System/chapter3-urdf-humanoids","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2: Digital Twin (Gazebo + Unity)","items":[{"type":"link","href":"/humanoid-robotics/docs/module2/Digital-Twin-Gazebo-Unity/","label":"Module 2: Digital Twin (Gazebo + Unity)","docId":"module2/Digital-Twin-Gazebo-Unity/index","unlisted":false},{"type":"link","href":"/humanoid-robotics/docs/module2/Digital-Twin-Gazebo-Unity/chapter1-gazebo-unity","label":"Chapter 1: Gazebo and Unity Integration","docId":"module2/Digital-Twin-Gazebo-Unity/chapter1-gazebo-unity","unlisted":false},{"type":"link","href":"/humanoid-robotics/docs/module2/Digital-Twin-Gazebo-Unity/chapter2-physics-simulation","label":"Chapter 2: Physics Simulation and Collisions","docId":"module2/Digital-Twin-Gazebo-Unity/chapter2-physics-simulation","unlisted":false},{"type":"link","href":"/humanoid-robotics/docs/module2/Digital-Twin-Gazebo-Unity/chapter3-sensors-simulation","label":"Chapter 3: Sensor Simulation","docId":"module2/Digital-Twin-Gazebo-Unity/chapter3-sensors-simulation","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3: NVIDIA Isaac (AI-Robot Brain)","items":[{"type":"link","href":"/humanoid-robotics/docs/module3/NVIDIA-Isaac-AI-Robot-Brain/","label":"Module 3: NVIDIA Isaac (AI-Robot Brain)","docId":"module3/NVIDIA-Isaac-AI-Robot-Brain/index","unlisted":false},{"type":"link","href":"/humanoid-robotics/docs/module3/NVIDIA-Isaac-AI-Robot-Brain/chapter1-isaac-sim","label":"Chapter 1: Isaac Sim and Synthetic Data","docId":"module3/NVIDIA-Isaac-AI-Robot-Brain/chapter1-isaac-sim","unlisted":false},{"type":"link","href":"/humanoid-robotics/docs/module3/NVIDIA-Isaac-AI-Robot-Brain/chapter2-vslam-navigation","label":"Chapter 2: VSLAM and Navigation","docId":"module3/NVIDIA-Isaac-AI-Robot-Brain/chapter2-vslam-navigation","unlisted":false},{"type":"link","href":"/humanoid-robotics/docs/module3/NVIDIA-Isaac-AI-Robot-Brain/chapter3-nav2-bipedal","label":"Chapter 3: Nav2 for Bipedal Path Planning","docId":"module3/NVIDIA-Isaac-AI-Robot-Brain/chapter3-nav2-bipedal","unlisted":false},{"type":"link","href":"/humanoid-robotics/docs/module3/NVIDIA-Isaac-AI-Robot-Brain/chapter4-sim-to-real","label":"Chapter 4: Sim-to-Real Transfer","docId":"module3/NVIDIA-Isaac-AI-Robot-Brain/chapter4-sim-to-real","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4: Vision-Language-Action (VLA) + Capstone","items":[{"type":"link","href":"/humanoid-robotics/docs/module4/Vision-Language-Action-VLA/","label":"Module 4: Vision-Language-Action (VLA) + Capstone","docId":"module4/Vision-Language-Action-VLA/index","unlisted":false},{"type":"link","href":"/humanoid-robotics/docs/module4/Vision-Language-Action-VLA/chapter1-whisper-commands","label":"Chapter 1: Whisper Voice Commands","docId":"module4/Vision-Language-Action-VLA/chapter1-whisper-commands","unlisted":false},{"type":"link","href":"/humanoid-robotics/docs/module4/Vision-Language-Action-VLA/chapter2-llm-planning","label":"Chapter 2: LLM-based Cognitive Planning","docId":"module4/Vision-Language-Action-VLA/chapter2-llm-planning","unlisted":false},{"type":"link","href":"/humanoid-robotics/docs/module4/Vision-Language-Action-VLA/chapter3-voice-to-action","label":"Chapter 3: Voice-to-Action Pipelines","docId":"module4/Vision-Language-Action-VLA/chapter3-voice-to-action","unlisted":false},{"type":"link","href":"/humanoid-robotics/docs/module4/Vision-Language-Action-VLA/chapter4-capstone-plagiarism-check","label":"Plagiarism Check for Capstone Integration Content","docId":"module4/Vision-Language-Action-VLA/chapter4-capstone-plagiarism-check","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"References","items":[{"type":"link","href":"/humanoid-robotics/docs/references/citations","label":"References and Citations","docId":"references/citations","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Tutorials","items":[{"type":"link","href":"/humanoid-robotics/docs/tutorials/getting-started","label":"Getting Started Tutorial","docId":"tutorials/getting-started","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"index":{"id":"index","title":"Physical AI & Humanoid Robotics","description":"This comprehensive guide covers the essential concepts and practical implementations for developing humanoid robots with artificial intelligence capabilities."},"intro":{"id":"intro","title":"Physical AI & Humanoid Robotics: A Comprehensive Guide","description":"Welcome to the Future of Robotics","sidebar":"tutorialSidebar"},"module1/ROS-2-Robotic-Nervous-System/chapter1-nodes-topics-services":{"id":"module1/ROS-2-Robotic-Nervous-System/chapter1-nodes-topics-services","title":"Chapter 1: Nodes, Topics, and Services","description":"This chapter covers the fundamental communication patterns in ROS 2: nodes, topics, and services that form the backbone of robotic systems.","sidebar":"tutorialSidebar"},"module1/ROS-2-Robotic-Nervous-System/chapter2-rclpy-integration":{"id":"module1/ROS-2-Robotic-Nervous-System/chapter2-rclpy-integration","title":"Chapter 2: rclpy Integration","description":"This chapter covers the Python client library for ROS 2 (rclpy) and how to integrate it with humanoid robotics applications.","sidebar":"tutorialSidebar"},"module1/ROS-2-Robotic-Nervous-System/chapter3-urdf-humanoids":{"id":"module1/ROS-2-Robotic-Nervous-System/chapter3-urdf-humanoids","title":"Chapter 3: URDF for Humanoids","description":"This chapter covers the Unified Robot Description Format (URDF) and its application to humanoid robot modeling in ROS 2.","sidebar":"tutorialSidebar"},"module1/ROS-2-Robotic-Nervous-System/citations-validation":{"id":"module1/ROS-2-Robotic-Nervous-System/citations-validation","title":"ROS 2 Citation Validation","description":"This document validates citations for the ROS 2 module content to ensure compliance with constitutional standards for academic rigor."},"module1/ROS-2-Robotic-Nervous-System/index":{"id":"module1/ROS-2-Robotic-Nervous-System/index","title":"Module 1: ROS 2 (Robotic Nervous System)","description":"This module covers the fundamentals of ROS 2 as the nervous system of robots, including nodes, topics, services, actions, and how to integrate rclpy agents with humanoid URDF models.","sidebar":"tutorialSidebar"},"module1/ROS-2-Robotic-Nervous-System/plagiarism-check":{"id":"module1/ROS-2-Robotic-Nervous-System/plagiarism-check","title":"ROS 2 Plagiarism Check","description":"This document ensures 0% plagiarism for the ROS 2 module content as required by constitutional standards."},"module2/Digital-Twin-Gazebo-Unity/chapter1-gazebo-unity":{"id":"module2/Digital-Twin-Gazebo-Unity/chapter1-gazebo-unity","title":"Chapter 1: Gazebo and Unity Integration","description":"This chapter covers the integration of Gazebo and Unity for creating comprehensive digital twin environments for humanoid robotics applications.","sidebar":"tutorialSidebar"},"module2/Digital-Twin-Gazebo-Unity/chapter2-physics-simulation":{"id":"module2/Digital-Twin-Gazebo-Unity/chapter2-physics-simulation","title":"Chapter 2: Physics Simulation and Collisions","description":"This chapter covers the implementation of physics simulation and collision detection for humanoid robots in digital twin environments.","sidebar":"tutorialSidebar"},"module2/Digital-Twin-Gazebo-Unity/chapter3-sensors-simulation":{"id":"module2/Digital-Twin-Gazebo-Unity/chapter3-sensors-simulation","title":"Chapter 3: Sensor Simulation","description":"This chapter covers the implementation and simulation of various sensors for humanoid robots, including LiDAR, depth cameras, and IMUs in digital twin environments.","sidebar":"tutorialSidebar"},"module2/Digital-Twin-Gazebo-Unity/citations-validation":{"id":"module2/Digital-Twin-Gazebo-Unity/citations-validation","title":"Digital Twin Citation Validation","description":"This document validates citations for the Digital Twin module content to ensure compliance with constitutional standards for academic rigor."},"module2/Digital-Twin-Gazebo-Unity/index":{"id":"module2/Digital-Twin-Gazebo-Unity/index","title":"Module 2: Digital Twin (Gazebo + Unity)","description":"This module covers creating and working with digital twin environments using Gazebo and Unity, including physics simulation, sensor modeling, and high-fidelity rendering for humanoid robotics applications.","sidebar":"tutorialSidebar"},"module2/Digital-Twin-Gazebo-Unity/plagiarism-check":{"id":"module2/Digital-Twin-Gazebo-Unity/plagiarism-check","title":"Digital Twin Plagiarism Check","description":"This document ensures 0% plagiarism for the Digital Twin module content as required by constitutional standards."},"module3/NVIDIA-Isaac-AI-Robot-Brain/chapter1-isaac-sim":{"id":"module3/NVIDIA-Isaac-AI-Robot-Brain/chapter1-isaac-sim","title":"Chapter 1: Isaac Sim and Synthetic Data","description":"This chapter covers NVIDIA Isaac Sim for generating synthetic data to train perception models for humanoid robots.","sidebar":"tutorialSidebar"},"module3/NVIDIA-Isaac-AI-Robot-Brain/chapter2-vslam-navigation":{"id":"module3/NVIDIA-Isaac-AI-Robot-Brain/chapter2-vslam-navigation","title":"Chapter 2: VSLAM and Navigation","description":"This chapter covers Visual Simultaneous Localization and Mapping (VSLAM) and navigation using NVIDIA Isaac ROS components for humanoid robots.","sidebar":"tutorialSidebar"},"module3/NVIDIA-Isaac-AI-Robot-Brain/chapter3-nav2-bipedal":{"id":"module3/NVIDIA-Isaac-AI-Robot-Brain/chapter3-nav2-bipedal","title":"Chapter 3: Nav2 for Bipedal Path Planning","description":"This chapter covers configuring and using the Navigation2 stack specifically for bipedal humanoid robot path planning and navigation.","sidebar":"tutorialSidebar"},"module3/NVIDIA-Isaac-AI-Robot-Brain/chapter4-sim-to-real":{"id":"module3/NVIDIA-Isaac-AI-Robot-Brain/chapter4-sim-to-real","title":"Chapter 4: Sim-to-Real Transfer","description":"This chapter covers the techniques and methodologies for transferring models and behaviors trained in simulation to real-world humanoid robots using NVIDIA Isaac platforms.","sidebar":"tutorialSidebar"},"module3/NVIDIA-Isaac-AI-Robot-Brain/citations-validation":{"id":"module3/NVIDIA-Isaac-AI-Robot-Brain/citations-validation","title":"NVIDIA Isaac Citation Validation","description":"This document validates citations for the NVIDIA Isaac module content to ensure compliance with constitutional standards for academic rigor."},"module3/NVIDIA-Isaac-AI-Robot-Brain/index":{"id":"module3/NVIDIA-Isaac-AI-Robot-Brain/index","title":"Module 3: NVIDIA Isaac (AI-Robot Brain)","description":"This module covers using NVIDIA Isaac for AI-powered robotics, including Isaac Sim for synthetic data generation, Isaac ROS for perception and navigation, and Nav2 for path planning.","sidebar":"tutorialSidebar"},"module3/NVIDIA-Isaac-AI-Robot-Brain/plagiarism-check":{"id":"module3/NVIDIA-Isaac-AI-Robot-Brain/plagiarism-check","title":"NVIDIA Isaac Plagiarism Check","description":"This document ensures 0% plagiarism for the NVIDIA Isaac module content as required by constitutional standards."},"module4/Vision-Language-Action-VLA/chapter1-whisper-commands":{"id":"module4/Vision-Language-Action-VLA/chapter1-whisper-commands","title":"Chapter 1: Whisper Voice Commands","description":"This chapter covers implementing voice-activated robotics systems using OpenAI's Whisper for voice command recognition in humanoid robots.","sidebar":"tutorialSidebar"},"module4/Vision-Language-Action-VLA/chapter2-llm-planning":{"id":"module4/Vision-Language-Action-VLA/chapter2-llm-planning","title":"Chapter 2: LLM-based Cognitive Planning","description":"This chapter covers implementing cognitive planning systems for humanoid robots using Large Language Models (LLMs) to process voice commands and generate executable action plans.","sidebar":"tutorialSidebar"},"module4/Vision-Language-Action-VLA/chapter3-voice-to-action":{"id":"module4/Vision-Language-Action-VLA/chapter3-voice-to-action","title":"Chapter 3: Voice-to-Action Pipelines","description":"This chapter covers the implementation of voice-to-action pipelines that convert spoken commands into executable robotic behaviors, integrating speech recognition, natural language processing, and action execution systems.","sidebar":"tutorialSidebar"},"module4/Vision-Language-Action-VLA/chapter4-capstone-plagiarism-check":{"id":"module4/Vision-Language-Action-VLA/chapter4-capstone-plagiarism-check","title":"Plagiarism Check for Capstone Integration Content","description":"This document ensures that all capstone integration content in the Vision-Language-Action module is original and properly attributed to meet constitutional requirements for academic integrity.","sidebar":"tutorialSidebar"},"module4/Vision-Language-Action-VLA/citations-validation":{"id":"module4/Vision-Language-Action-VLA/citations-validation","title":"Citation Validation for Vision-Language-Action (VLA) Module","description":"This document validates that all citations in the Vision-Language-Action module meet constitutional requirements for academic rigor and credibility."},"module4/Vision-Language-Action-VLA/index":{"id":"module4/Vision-Language-Action-VLA/index","title":"Module 4: Vision-Language-Action (VLA) + Capstone","description":"This module covers implementing voice-activated robotics systems using Whisper for voice commands, LLMs for cognitive planning, and voice-to-action pipelines, plus the capstone integration project.","sidebar":"tutorialSidebar"},"module4/Vision-Language-Action-VLA/plagiarism-check":{"id":"module4/Vision-Language-Action-VLA/plagiarism-check","title":"Plagiarism Check for Vision-Language-Action (VLA) Module","description":"This document ensures that all content in the Vision-Language-Action module is original and properly attributed to meet constitutional requirements for academic integrity."},"references/citations":{"id":"references/citations","title":"References and Citations","description":"This document contains the comprehensive list of references used throughout the Physical AI & Humanoid Robotics book, following APA citation format as required by constitutional standards.","sidebar":"tutorialSidebar"},"tutorials/getting-started":{"id":"tutorials/getting-started","title":"Getting Started Tutorial","description":"This tutorial provides a step-by-step introduction to the Physical AI & Humanoid Robotics book and helps you set up your development environment.","sidebar":"tutorialSidebar"}}}}