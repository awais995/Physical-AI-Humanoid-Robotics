"use strict";(globalThis.webpackChunkphysical_ai_humanoid_book=globalThis.webpackChunkphysical_ai_humanoid_book||[]).push([[863],{6310:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module4/Vision-Language-Action-VLA/chapter3-voice-to-action","title":"Chapter 3: Voice-to-Action Pipelines","description":"This chapter covers the implementation of voice-to-action pipelines that convert spoken commands into executable robotic behaviors, integrating speech recognition, natural language processing, and action execution systems.","source":"@site/docs/module4/Vision-Language-Action-VLA/chapter3-voice-to-action.md","sourceDirName":"module4/Vision-Language-Action-VLA","slug":"/module4/Vision-Language-Action-VLA/chapter3-voice-to-action","permalink":"/humanoid-robotics/docs/module4/Vision-Language-Action-VLA/chapter3-voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/your-project-name/tree/main/docs/module4/Vision-Language-Action-VLA/chapter3-voice-to-action.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: LLM-based Cognitive Planning","permalink":"/humanoid-robotics/docs/module4/Vision-Language-Action-VLA/chapter2-llm-planning"},"next":{"title":"Plagiarism Check for Capstone Integration Content","permalink":"/humanoid-robotics/docs/module4/Vision-Language-Action-VLA/chapter4-capstone-plagiarism-check"}}');var a=t(4848),o=t(8453);const s={sidebar_position:4},r="Chapter 3: Voice-to-Action Pipelines",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Voice-to-Action Pipelines",id:"introduction-to-voice-to-action-pipelines",level:2},{value:"Architecture of Voice-to-Action Systems",id:"architecture-of-voice-to-action-systems",level:2},{value:"Pipeline Components",id:"pipeline-components",level:3},{value:"Pipeline Configuration and Management",id:"pipeline-configuration-and-management",level:3},{value:"Natural Language Understanding for Robotics",id:"natural-language-understanding-for-robotics",level:2},{value:"Intent Recognition and Classification",id:"intent-recognition-and-classification",level:3},{value:"Context-Aware Command Processing",id:"context-aware-command-processing",level:3},{value:"Action Mapping and Execution",id:"action-mapping-and-execution",level:2},{value:"Action Planning and Sequencing",id:"action-planning-and-sequencing",level:3},{value:"Execution Validation and Safety",id:"execution-validation-and-safety",level:3},{value:"Real-time Processing and Optimization",id:"real-time-processing-and-optimization",level:2},{value:"Async Pipeline Implementation",id:"async-pipeline-implementation",level:3},{value:"Integration with Robot Systems",id:"integration-with-robot-systems",level:2},{value:"ROS Integration for Voice Commands",id:"ros-integration-for-voice-commands",level:3},{value:"Error Handling and Recovery",id:"error-handling-and-recovery",level:2},{value:"Robust Error Management",id:"robust-error-management",level:3},{value:"Research Tasks",id:"research-tasks",level:2},{value:"Evidence Requirements",id:"evidence-requirements",level:2},{value:"References",id:"references",level:2},{value:"Practical Exercises",id:"practical-exercises",level:2}];function d(n){const e={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"chapter-3-voice-to-action-pipelines",children:"Chapter 3: Voice-to-Action Pipelines"})}),"\n",(0,a.jsx)(e.p,{children:"This chapter covers the implementation of voice-to-action pipelines that convert spoken commands into executable robotic behaviors, integrating speech recognition, natural language processing, and action execution systems."}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(e.p,{children:"After completing this chapter, students will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Design and implement voice-to-action pipeline architectures"}),"\n",(0,a.jsx)(e.li,{children:"Integrate speech recognition with action planning systems"}),"\n",(0,a.jsx)(e.li,{children:"Create robust command interpretation and validation mechanisms"}),"\n",(0,a.jsx)(e.li,{children:"Implement error handling and recovery for voice-commanded actions"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"introduction-to-voice-to-action-pipelines",children:"Introduction to Voice-to-Action Pipelines"}),"\n",(0,a.jsx)(e.p,{children:"Voice-to-action pipelines form the bridge between human speech and robot behavior execution. These systems process natural language commands and transform them into sequences of executable robotic actions, requiring sophisticated integration of multiple technologies including speech recognition, natural language understanding, and action planning."}),"\n",(0,a.jsx)(e.p,{children:"The key components of a voice-to-action pipeline include:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Speech recognition (converting audio to text)"}),"\n",(0,a.jsx)(e.li,{children:"Natural language understanding (interpreting command intent)"}),"\n",(0,a.jsx)(e.li,{children:"Action mapping (translating intent to robot actions)"}),"\n",(0,a.jsx)(e.li,{children:"Execution validation (ensuring safe and feasible actions)"}),"\n",(0,a.jsx)(e.li,{children:"Feedback generation (confirming or clarifying commands)"}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"[@wu2021; @kumar2022]"}),"\n",(0,a.jsx)(e.h2,{id:"architecture-of-voice-to-action-systems",children:"Architecture of Voice-to-Action Systems"}),"\n",(0,a.jsx)(e.h3,{id:"pipeline-components",children:"Pipeline Components"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Voice-to-action pipeline architecture\nimport asyncio\nimport threading\nfrom typing import Dict, List, Optional, Callable, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport time\nimport logging\n\nclass CommandStatus(Enum):\n    PENDING = "pending"\n    PROCESSING = "processing"\n    VALIDATED = "validated"\n    EXECUTING = "executing"\n    COMPLETED = "completed"\n    FAILED = "failed"\n    REJECTED = "rejected"\n\n@dataclass\nclass VoiceCommand:\n    """Represents a voice command with metadata"""\n    raw_audio: Optional[bytes] = None\n    transcribed_text: str = ""\n    confidence: float = 0.0\n    timestamp: float = 0.0\n    intent: Optional[str] = None\n    parameters: Dict[str, Any] = None\n    status: CommandStatus = CommandStatus.PENDING\n    action_sequence: List[Dict[str, Any]] = None\n\n@dataclass\nclass RobotAction:\n    """Represents a single robot action"""\n    action_type: str\n    parameters: Dict[str, Any]\n    description: str\n    priority: int = 1\n    estimated_duration: float = 1.0\n\nclass VoiceToActionPipeline:\n    def __init__(self):\n        self.speech_recognizer = None  # Would be Whisper or similar\n        self.nlu_processor = None     # Natural Language Understanding\n        self.action_mapper = None     # Maps intents to actions\n        self.validator = None         # Validates actions\n        self.executor = None          # Executes actions\n\n        # Pipeline state\n        self.command_queue = asyncio.Queue()\n        self.active_commands = {}\n        self.command_history = []\n\n        # Callbacks\n        self.on_command_received = None\n        self.on_command_processed = None\n        self.on_action_executed = None\n\n    async def process_voice_command(self, audio_input: bytes) -> Optional[VoiceCommand]:\n        """Process voice command through the entire pipeline"""\n        command = VoiceCommand(\n            raw_audio=audio_input,\n            timestamp=time.time()\n        )\n\n        try:\n            # Step 1: Speech recognition\n            command.transcribed_text = await self._recognize_speech(audio_input)\n            command.status = CommandStatus.PROCESSING\n\n            # Step 2: Natural language understanding\n            intent, params = await self._understand_intent(command.transcribed_text)\n            command.intent = intent\n            command.parameters = params\n\n            # Step 3: Action mapping\n            action_sequence = await self._map_to_actions(intent, params)\n            command.action_sequence = action_sequence\n            command.status = CommandStatus.VALIDATED\n\n            # Step 4: Validation\n            is_valid, validation_errors = await self._validate_actions(action_sequence)\n            if not is_valid:\n                command.status = CommandStatus.REJECTED\n                logging.error(f"Command rejected: {validation_errors}")\n                return command\n\n            # Step 5: Execution\n            success = await self._execute_actions(action_sequence)\n            command.status = CommandStatus.COMPLETED if success else CommandStatus.FAILED\n\n            # Add to history\n            self.command_history.append(command)\n\n            return command\n\n        except Exception as e:\n            logging.error(f"Error in voice-to-action pipeline: {e}")\n            command.status = CommandStatus.FAILED\n            return command\n\n    async def _recognize_speech(self, audio_data: bytes) -> str:\n        """Convert audio to text using speech recognition"""\n        # In a real implementation, this would use Whisper or similar\n        # For now, we\'ll simulate the process\n        await asyncio.sleep(0.1)  # Simulate processing time\n        return "move forward 2 meters"  # Simulated result\n\n    async def _understand_intent(self, text: str) -> tuple[str, Dict[str, Any]]:\n        """Extract intent and parameters from text command"""\n        # Simple rule-based intent extraction\n        # In practice, this would use NLU models\n        text_lower = text.lower()\n\n        if "move" in text_lower or "go" in text_lower:\n            intent = "navigation"\n            params = self._extract_navigation_params(text_lower)\n        elif "grasp" in text_lower or "pick" in text_lower:\n            intent = "manipulation"\n            params = self._extract_manipulation_params(text_lower)\n        elif "speak" in text_lower or "say" in text_lower:\n            intent = "communication"\n            params = self._extract_communication_params(text_lower)\n        else:\n            intent = "unknown"\n            params = {"text": text}\n\n        return intent, params\n\n    def _extract_navigation_params(self, text: str) -> Dict[str, Any]:\n        """Extract navigation parameters from text"""\n        params = {"direction": "forward", "distance": 1.0}\n\n        # Extract distance\n        import re\n        distance_match = re.search(r\'(\\d+(?:\\.\\d+)?)\\s*(meters?|m)\', text)\n        if distance_match:\n            params["distance"] = float(distance_match.group(1))\n\n        # Extract direction\n        if "backward" in text or "back" in text:\n            params["direction"] = "backward"\n        elif "left" in text:\n            params["direction"] = "left"\n        elif "right" in text:\n            params["direction"] = "right"\n        elif "forward" in text:\n            params["direction"] = "forward"\n\n        return params\n\n    def _extract_manipulation_params(self, text: str) -> Dict[str, Any]:\n        """Extract manipulation parameters from text"""\n        params = {"object": "unknown", "action": "grasp"}\n\n        # Extract object name\n        import re\n        object_match = re.search(r\'(?:pick up|grasp|hold|take)\\s+(.+?)\\s*$\', text)\n        if object_match:\n            params["object"] = object_match.group(1).strip()\n\n        return params\n\n    def _extract_communication_params(self, text: str) -> Dict[str, Any]:\n        """Extract communication parameters from text"""\n        params = {"message": text}\n\n        # Extract just the message part if it contains "say" or similar\n        import re\n        say_match = re.search(r\'(?:say|speak|tell)\\s+(.+?)\\s*$\', text)\n        if say_match:\n            params["message"] = say_match.group(1).strip()\n\n        return params\n\n    async def _map_to_actions(self, intent: str, params: Dict[str, Any]) -> List[RobotAction]:\n        """Map intent and parameters to executable robot actions"""\n        action_map = {\n            "navigation": self._create_navigation_actions,\n            "manipulation": self._create_manipulation_actions,\n            "communication": self._create_communication_actions\n        }\n\n        action_creator = action_map.get(intent, self._create_default_actions)\n        return action_creator(params)\n\n    def _create_navigation_actions(self, params: Dict[str, Any]) -> List[RobotAction]:\n        """Create navigation actions from parameters"""\n        actions = []\n\n        # Move action\n        actions.append(RobotAction(\n            action_type="move_base",\n            parameters={\n                "direction": params.get("direction", "forward"),\n                "distance": params.get("distance", 1.0),\n                "speed": 0.5\n            },\n            description=f"Move {params.get(\'direction\', \'forward\')} {params.get(\'distance\', 1.0)} meters",\n            estimated_duration=params.get("distance", 1.0) * 2  # Estimate based on distance\n        ))\n\n        return actions\n\n    def _create_manipulation_actions(self, params: Dict[str, Any]) -> List[RobotAction]:\n        """Create manipulation actions from parameters"""\n        actions = []\n\n        # Detect object\n        actions.append(RobotAction(\n            action_type="detect_object",\n            parameters={"target_object": params.get("object", "unknown")},\n            description=f"Detect {params.get(\'object\', \'unknown\')} object",\n            estimated_duration=2.0\n        ))\n\n        # Move to object\n        actions.append(RobotAction(\n            action_type="move_to_object",\n            parameters={"object_name": params.get("object", "unknown")},\n            description=f"Move to {params.get(\'object\', \'unknown\')} object",\n            estimated_duration=5.0\n        ))\n\n        # Grasp object\n        actions.append(RobotAction(\n            action_type="grasp_object",\n            parameters={"object_name": params.get("object", "unknown")},\n            description=f"Grasp {params.get(\'object\', \'unknown\')} object",\n            estimated_duration=3.0\n        ))\n\n        return actions\n\n    def _create_communication_actions(self, params: Dict[str, Any]) -> List[RobotAction]:\n        """Create communication actions from parameters"""\n        actions = []\n\n        actions.append(RobotAction(\n            action_type="speak",\n            parameters={"text": params.get("message", "Hello")},\n            description=f"Speak: {params.get(\'message\', \'Hello\')}",\n            estimated_duration=len(params.get("message", "Hello").split()) * 0.5\n        ))\n\n        return actions\n\n    def _create_default_actions(self, params: Dict[str, Any]) -> List[RobotAction]:\n        """Create default actions when intent is unknown"""\n        return [\n            RobotAction(\n                action_type="unknown_command",\n                parameters=params,\n                description="Unknown command - no action",\n                estimated_duration=0.0\n            )\n        ]\n\n    async def _validate_actions(self, actions: List[RobotAction]) -> tuple[bool, List[str]]:\n        """Validate that actions are safe and executable"""\n        errors = []\n\n        for action in actions:\n            # Check for dangerous actions\n            if action.action_type in ["dangerous_action", "unsafe_move"]:\n                errors.append(f"Dangerous action: {action.action_type}")\n\n            # Check parameter validity\n            if action.action_type == "move_base":\n                distance = action.parameters.get("distance", 0)\n                if distance > 10:  # Arbitrary limit\n                    errors.append(f"Movement distance too large: {distance}m")\n\n        return len(errors) == 0, errors\n\n    async def _execute_actions(self, actions: List[RobotAction]) -> bool:\n        """Execute the action sequence on the robot"""\n        for action in actions:\n            success = await self._execute_single_action(action)\n            if not success:\n                return False\n\n        return True\n\n    async def _execute_single_action(self, action: RobotAction) -> bool:\n        """Execute a single robot action"""\n        # In a real implementation, this would interface with the robot\n        # For simulation, we\'ll just print and sleep\n        print(f"Executing: {action.action_type} - {action.description}")\n\n        # Simulate action execution time\n        await asyncio.sleep(action.estimated_duration)\n\n        return True  # Simulate success\n'})}),"\n",(0,a.jsx)(e.p,{children:"[@mohamed2020; @li2021]"}),"\n",(0,a.jsx)(e.h3,{id:"pipeline-configuration-and-management",children:"Pipeline Configuration and Management"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class PipelineConfig:\n    """Configuration for voice-to-action pipeline"""\n    def __init__(self):\n        # Speech recognition settings\n        self.speech_model = "whisper-base"\n        self.audio_sample_rate = 16000\n        self.audio_channels = 1\n        self.vad_threshold = 0.3  # Voice activity detection threshold\n        self.silence_duration = 1.0  # Seconds of silence to trigger processing\n\n        # Natural language understanding settings\n        self.nlu_model = "gpt-3.5-turbo"\n        self.intent_confidence_threshold = 0.7\n        self.max_command_length = 100  # Maximum characters in command\n\n        # Action execution settings\n        self.max_action_sequence_length = 10\n        self.action_timeout = 30.0  # Seconds before action timeout\n        self.execution_retries = 3\n\n        # Safety settings\n        self.safety_keywords = ["dangerous", "unsafe", "harm"]\n        self.max_movement_distance = 5.0  # Meters\n\nclass VoiceToActionManager:\n    """Manages multiple voice-to-action pipelines"""\n    def __init__(self, config: PipelineConfig):\n        self.config = config\n        self.pipelines = {}\n        self.active_pipeline_id = None\n        self.pipeline_counter = 0\n\n        # Initialize default pipeline\n        self.add_pipeline("default")\n\n    def add_pipeline(self, name: str) -> str:\n        """Add a new voice-to-action pipeline"""\n        pipeline_id = f"pipeline_{self.pipeline_counter}"\n        self.pipeline_counter += 1\n\n        pipeline = VoiceToActionPipeline()\n        # Configure pipeline with settings\n        self.pipelines[pipeline_id] = {\n            "name": name,\n            "pipeline": pipeline,\n            "status": "ready",\n            "created_at": time.time()\n        }\n\n        return pipeline_id\n\n    def remove_pipeline(self, pipeline_id: str):\n        """Remove a voice-to-action pipeline"""\n        if pipeline_id in self.pipelines:\n            del self.pipelines[pipeline_id]\n\n    async def process_command(self, audio_input: bytes, pipeline_id: str = None) -> Optional[VoiceCommand]:\n        """Process command using specified or default pipeline"""\n        if pipeline_id is None:\n            pipeline_id = self.active_pipeline_id or next(iter(self.pipelines.keys()))\n\n        if pipeline_id not in self.pipelines:\n            raise ValueError(f"Pipeline {pipeline_id} not found")\n\n        pipeline = self.pipelines[pipeline_id]["pipeline"]\n        return await pipeline.process_voice_command(audio_input)\n\n    def get_pipeline_status(self, pipeline_id: str) -> Dict[str, Any]:\n        """Get status of a specific pipeline"""\n        if pipeline_id not in self.pipelines:\n            return {}\n\n        pipeline_info = self.pipelines[pipeline_id]\n        return {\n            "id": pipeline_id,\n            "name": pipeline_info["name"],\n            "status": pipeline_info["status"],\n            "active_commands": len(pipeline_info["pipeline"].active_commands),\n            "command_history_size": len(pipeline_info["pipeline"].command_history)\n        }\n\n    def set_active_pipeline(self, pipeline_id: str):\n        """Set the active pipeline for default processing"""\n        if pipeline_id in self.pipelines:\n            self.active_pipeline_id = pipeline_id\n        else:\n            raise ValueError(f"Pipeline {pipeline_id} not found")\n'})}),"\n",(0,a.jsx)(e.p,{children:"[@wang2022; @zhang2023]"}),"\n",(0,a.jsx)(e.h2,{id:"natural-language-understanding-for-robotics",children:"Natural Language Understanding for Robotics"}),"\n",(0,a.jsx)(e.h3,{id:"intent-recognition-and-classification",children:"Intent Recognition and Classification"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import re\nfrom typing import Tuple\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nimport numpy as np\n\nclass IntentClassifier:\n    """Classifies user intents for robotic commands"""\n    def __init__(self):\n        self.vectorizer = TfidfVectorizer(\n            lowercase=True,\n            stop_words=\'english\',\n            ngram_range=(1, 2)\n        )\n        self.classifier = MultinomialNB()\n\n        # Define intent patterns and training data\n        self.intent_patterns = {\n            \'navigation\': [\n                r\'move\\s+(?P<direction>\\w+)\\s*(?P<distance>\\d*\\.?\\d*)\\s*(m|meter|meters)?\',\n                r\'go\\s+(?P<direction>\\w+)\\s*(?P<distance>\\d*\\.?\\d*)\\s*(m|meter|meters)?\',\n                r\'walk\\s+(?P<direction>\\w+)\\s*(?P<distance>\\d*\\.?\\d*)\\s*(m|meter|meters)?\',\n                r\'turn\\s+(?P<direction>\\w+)\',\n                r\'navigate\\s+to\\s+(?P<location>[\\w\\s]+)\'\n            ],\n            \'manipulation\': [\n                r\'pick\\s+up\\s+(?P<object>[\\w\\s]+)\',\n                r\'grasp\\s+(?P<object>[\\w\\s]+)\',\n                r\'grab\\s+(?P<object>[\\w\\s]+)\',\n                r\'hold\\s+(?P<object>[\\w\\s]+)\',\n                r\'place\\s+(?P<object>[\\w\\s]+)\\s+on\\s+(?P<surface>[\\w\\s]+)\'\n            ],\n            \'communication\': [\n                r\'say\\s+(?P<message>[\\w\\s]+)\',\n                r\'speak\\s+(?P<message>[\\w\\s]+)\',\n                r\'tell\\s+(?P<message>[\\w\\s]+)\',\n                r\'greet\\s+(?P<target>[\\w\\s]+)\',\n                r\'hello\',\n                r\'hi\'\n            ],\n            \'query\': [\n                r\'where\\s+is\\s+(?P<target>[\\w\\s]+)\',\n                r\'find\\s+(?P<target>[\\w\\s]+)\',\n                r\'locate\\s+(?P<target>[\\w\\s]+)\',\n                r\'what\\s+is\\s+this\',\n                r\'who\\s+are\\s+you\'\n            ]\n        }\n\n        # Training data for ML classifier\n        self.training_sentences = [\n            ("Move forward 2 meters", "navigation"),\n            ("Go left", "navigation"),\n            ("Turn right", "navigation"),\n            ("Navigate to kitchen", "navigation"),\n            ("Pick up the red cup", "manipulation"),\n            ("Grasp the book", "manipulation"),\n            ("Place object on table", "manipulation"),\n            ("Say hello world", "communication"),\n            ("Tell me a joke", "communication"),\n            ("Where is the chair?", "query"),\n            ("Find the remote", "query"),\n        ]\n\n        self._train_classifier()\n\n    def _train_classifier(self):\n        """Train the intent classifier with sample data"""\n        if not self.training_sentences:\n            return\n\n        sentences, labels = zip(*self.training_sentences)\n        X = self.vectorizer.fit_transform(sentences)\n        self.classifier.fit(X, labels)\n\n    def classify_intent(self, text: str) -> Tuple[str, Dict[str, str], float]:\n        """Classify intent and extract parameters from text"""\n        # First, try pattern matching\n        for intent, patterns in self.intent_patterns.items():\n            for pattern in patterns:\n                match = re.search(pattern, text.lower())\n                if match:\n                    return intent, match.groupdict(), 1.0  # High confidence for pattern match\n\n        # If no pattern matches, use ML classifier\n        X = self.vectorizer.transform([text])\n        predicted_intent = self.classifier.predict(X)[0]\n        confidence = max(self.classifier.predict_proba(X)[0])\n\n        return predicted_intent, {}, confidence\n\n    def extract_parameters(self, text: str, intent: str) -> Dict[str, str]:\n        """Extract specific parameters for the given intent"""\n        # Use patterns to extract parameters\n        for pattern in self.intent_patterns.get(intent, []):\n            match = re.search(pattern, text.lower())\n            if match:\n                return match.groupdict()\n\n        # If no pattern matches, return empty dict\n        return {}\n'})}),"\n",(0,a.jsx)(e.p,{children:"[@devlin2019; @mikolov2013]"}),"\n",(0,a.jsx)(e.h3,{id:"context-aware-command-processing",children:"Context-Aware Command Processing"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class ContextAwareProcessor:\n    """Processes commands with context awareness"""\n    def __init__(self):\n        self.context_history = []\n        self.max_context_length = 10\n        self.location_context = {}\n        self.object_context = {}\n        self.user_context = {}\n        self.task_context = {}\n\n    def add_context(self, context_type: str, data: Dict[str, Any]):\n        """Add context information"""\n        context_entry = {\n            "type": context_type,\n            "data": data,\n            "timestamp": time.time()\n        }\n\n        self.context_history.append(context_entry)\n\n        # Keep history within limits\n        if len(self.context_history) > self.max_context_length:\n            self.context_history.pop(0)\n\n        # Update specific context stores\n        if context_type == "location":\n            self.location_context.update(data)\n        elif context_type == "object":\n            self.object_context.update(data)\n        elif context_type == "user":\n            self.user_context.update(data)\n        elif context_type == "task":\n            self.task_context.update(data)\n\n    def resolve_command_context(self, command: str) -> str:\n        """Resolve ambiguous commands using context"""\n        # Example: "Go there" -> "Go to living room"\n        # Example: "Pick it up" -> "Pick up red cup"\n\n        resolved_command = command\n\n        # Handle "it" reference\n        if "it" in command.lower() and self.object_context:\n            last_object = self.object_context.get("last_detected_object", "object")\n            resolved_command = command.lower().replace("it", last_object)\n\n        # Handle "there" reference\n        if "there" in command.lower() and self.location_context:\n            last_location = self.location_context.get("last_visited", "destination")\n            resolved_command = command.lower().replace("there", last_location)\n\n        return resolved_command\n\n    def get_relevant_context(self) -> Dict[str, Any]:\n        """Get relevant context for current command"""\n        return {\n            "location": self.location_context,\n            "objects": self.object_context,\n            "user": self.user_context,\n            "task": self.task_context,\n            "recent_actions": self.context_history[-5:] if self.context_history else []\n        }\n'})}),"\n",(0,a.jsx)(e.p,{children:"[@bordes2016; @weston2015]"}),"\n",(0,a.jsx)(e.h2,{id:"action-mapping-and-execution",children:"Action Mapping and Execution"}),"\n",(0,a.jsx)(e.h3,{id:"action-planning-and-sequencing",children:"Action Planning and Sequencing"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class ActionPlanner:\n    """Plans sequences of actions for complex commands"""\n    def __init__(self):\n        self.action_library = {\n            "navigation": [\n                "move_to", "avoid_obstacles", "navigate_to_location",\n                "turn_left", "turn_right", "move_forward", "move_backward"\n            ],\n            "manipulation": [\n                "detect_object", "approach_object", "grasp_object",\n                "release_object", "place_object", "move_arm_to_position"\n            ],\n            "communication": [\n                "speak", "listen", "face_person", "greet", "respond"\n            ],\n            "perception": [\n                "detect_person", "detect_object", "recognize_speaker",\n                "scan_environment", "identify_obstacle"\n            ]\n        }\n\n        self.action_dependencies = {\n            "grasp_object": ["detect_object", "approach_object"],\n            "place_object": ["grasp_object", "navigate_to_location"],\n            "respond": ["listen", "recognize_speaker"]\n        }\n\n    def plan_actions(self, intent: str, parameters: Dict[str, Any]) -> List[RobotAction]:\n        """Plan sequence of actions for given intent and parameters"""\n        if intent == "complex_task":\n            return self._plan_complex_task(parameters)\n        elif intent == "navigation":\n            return self._plan_navigation_task(parameters)\n        elif intent == "manipulation":\n            return self._plan_manipulation_task(parameters)\n        else:\n            return self._plan_simple_task(intent, parameters)\n\n    def _plan_navigation_task(self, params: Dict[str, Any]) -> List[RobotAction]:\n        """Plan navigation-specific actions"""\n        actions = []\n\n        # Check if we need to find the target location\n        target_location = params.get("location", "unknown")\n        if target_location in ["kitchen", "living room", "bedroom", "unknown"]:\n            # Need to localize the location\n            actions.append(RobotAction(\n                action_type="find_location",\n                parameters={"location": target_location},\n                description=f"Find {target_location} location",\n                estimated_duration=5.0\n            ))\n\n        # Add movement action\n        actions.append(RobotAction(\n            action_type="move_to",\n            parameters={\n                "target": target_location,\n                "speed": params.get("speed", 0.5)\n            },\n            description=f"Move to {target_location}",\n            estimated_duration=10.0  # Estimate based on distance\n        ))\n\n        return actions\n\n    def _plan_manipulation_task(self, params: Dict[str, Any]) -> List[RobotAction]:\n        """Plan manipulation-specific actions"""\n        actions = []\n\n        target_object = params.get("object", "unknown")\n\n        # Detect the object\n        actions.append(RobotAction(\n            action_type="detect_object",\n            parameters={"target_object": target_object},\n            description=f"Detect {target_object}",\n            estimated_duration=3.0\n        ))\n\n        # Approach the object\n        actions.append(RobotAction(\n            action_type="approach_object",\n            parameters={"target_object": target_object},\n            description=f"Approach {target_object}",\n            estimated_duration=5.0\n        ))\n\n        # Grasp the object\n        actions.append(RobotAction(\n            action_type="grasp_object",\n            parameters={"target_object": target_object},\n            description=f"Grasp {target_object}",\n            estimated_duration=4.0\n        ))\n\n        return actions\n\n    def _plan_complex_task(self, params: Dict[str, Any]) -> List[RobotAction]:\n        """Plan complex multi-step tasks"""\n        # Example: "Go to kitchen and bring me a cup"\n        subtasks = params.get("subtasks", [])\n        all_actions = []\n\n        for subtask in subtasks:\n            subtask_actions = self.plan_actions(subtask["intent"], subtask["parameters"])\n            all_actions.extend(subtask_actions)\n\n        return all_actions\n\n    def _plan_simple_task(self, intent: str, params: Dict[str, Any]) -> List[RobotAction]:\n        """Plan simple, single-action tasks"""\n        # Default simple action mapping\n        action_type = intent.replace(" ", "_").lower()\n\n        return [RobotAction(\n            action_type=action_type,\n            parameters=params,\n            description=f"{intent} with params {params}",\n            estimated_duration=2.0\n        )]\n\n    def validate_action_sequence(self, actions: List[RobotAction]) -> Tuple[bool, List[str]]:\n        """Validate that action sequence is feasible"""\n        errors = []\n\n        # Check dependencies\n        completed_actions = set()\n        for i, action in enumerate(actions):\n            required_actions = self.action_dependencies.get(action.action_type, [])\n\n            for req_action in required_actions:\n                if req_action not in completed_actions:\n                    errors.append(f"Action {i} ({action.action_type}) requires {req_action} which is not completed")\n\n            completed_actions.add(action.action_type)\n\n        # Check action validity\n        for action in actions:\n            valid_actions = []\n            for action_list in self.action_library.values():\n                valid_actions.extend(action_list)\n\n            if action.action_type not in valid_actions:\n                errors.append(f"Unknown action type: {action.action_type}")\n\n        return len(errors) == 0, errors\n'})}),"\n",(0,a.jsx)(e.p,{children:"[@fox2017; @ghallab2004]"}),"\n",(0,a.jsx)(e.h3,{id:"execution-validation-and-safety",children:"Execution Validation and Safety"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class ExecutionValidator:\n    """Validates action execution for safety and feasibility"""\n    def __init__(self):\n        self.safety_constraints = {\n            "movement": {\n                "max_distance": 10.0,  # meters\n                "max_speed": 1.0,      # m/s\n                "min_obstacle_distance": 0.5  # meters\n            },\n            "manipulation": {\n                "max_object_weight": 5.0,  # kg\n                "reachable_distance": 1.5  # meters\n            }\n        }\n\n        self.environment_context = {\n            "obstacles": [],\n            "people": [],\n            "safe_zones": [],\n            "restricted_areas": []\n        }\n\n    def validate_action(self, action: RobotAction, env_context: Dict[str, Any] = None) -> Tuple[bool, List[str]]:\n        """Validate a single action against safety constraints"""\n        if env_context:\n            self.environment_context.update(env_context)\n\n        errors = []\n\n        if action.action_type in ["move_to", "navigate", "move_forward", "move_backward"]:\n            errors.extend(self._validate_movement_action(action))\n        elif action.action_type in ["grasp_object", "pick_up", "place_object"]:\n            errors.extend(self._validate_manipulation_action(action))\n        elif action.action_type == "speak":\n            errors.extend(self._validate_communication_action(action))\n\n        return len(errors) == 0, errors\n\n    def _validate_movement_action(self, action: RobotAction) -> List[str]:\n        """Validate movement actions"""\n        errors = []\n        params = action.parameters\n\n        # Check distance constraints\n        distance = params.get("distance", 0)\n        if distance > self.safety_constraints["movement"]["max_distance"]:\n            errors.append(f"Movement distance {distance}m exceeds maximum {self.safety_constraints[\'movement\'][\'max_distance\']}m")\n\n        # Check speed constraints\n        speed = params.get("speed", 0.5)\n        if speed > self.safety_constraints["movement"]["max_speed"]:\n            errors.append(f"Movement speed {speed}m/s exceeds maximum {self.safety_constraints[\'movement\'][\'max_speed\']}m/s")\n\n        # Check for obstacles in path\n        target_location = params.get("target")\n        if target_location and self._is_path_blocked(target_location):\n            errors.append(f"Path to {target_location} is blocked by obstacles")\n\n        # Check if destination is in restricted area\n        if target_location and self._is_restricted_area(target_location):\n            errors.append(f"Destination {target_location} is in a restricted area")\n\n        return errors\n\n    def _validate_manipulation_action(self, action: RobotAction) -> List[str]:\n        """Validate manipulation actions"""\n        errors = []\n        params = action.parameters\n\n        # Check object weight (if known)\n        object_name = params.get("target_object", "unknown")\n        object_weight = self._get_object_weight(object_name)\n\n        if object_weight and object_weight > self.safety_constraints["manipulation"]["max_object_weight"]:\n            errors.append(f"Object {object_name} weighs {object_weight}kg which exceeds maximum {self.safety_constraints[\'manipulation\'][\'max_object_weight\']}kg")\n\n        # Check reachability\n        object_distance = self._get_object_distance(object_name)\n        if object_distance and object_distance > self.safety_constraints["manipulation"]["reachable_distance"]:\n            errors.append(f"Object {object_name} is {object_distance}m away which exceeds reachable distance of {self.safety_constraints[\'manipulation\'][\'reachable_distance\']}m")\n\n        return errors\n\n    def _validate_communication_action(self, action: RobotAction) -> List[str]:\n        """Validate communication actions"""\n        errors = []\n        params = action.parameters\n\n        # Check for inappropriate content\n        message = params.get("text", "")\n        if self._contains_inappropriate_content(message):\n            errors.append("Message contains inappropriate content")\n\n        return errors\n\n    def _is_path_blocked(self, target_location: str) -> bool:\n        """Check if path to target location is blocked"""\n        # In a real implementation, this would check navigation maps\n        # For now, we\'ll return False\n        return False\n\n    def _is_restricted_area(self, location: str) -> bool:\n        """Check if location is in restricted area"""\n        return location.lower() in [area.lower() for area in self.environment_context.get("restricted_areas", [])]\n\n    def _get_object_weight(self, object_name: str) -> Optional[float]:\n        """Get known weight of object"""\n        # In a real implementation, this would query an object database\n        # For now, return None (unknown)\n        return None\n\n    def _get_object_distance(self, object_name: str) -> Optional[float]:\n        """Get distance to object"""\n        # In a real implementation, this would query perception system\n        # For now, return None (unknown)\n        return None\n\n    def _contains_inappropriate_content(self, message: str) -> bool:\n        """Check if message contains inappropriate content"""\n        # Simple keyword check (in practice, this would be more sophisticated)\n        inappropriate_keywords = ["inappropriate", "offensive", "harmful"]\n        return any(keyword in message.lower() for keyword in inappropriate_keywords)\n\n    def validate_action_sequence(self, actions: List[RobotAction], env_context: Dict[str, Any] = None) -> Tuple[bool, List[str]]:\n        """Validate entire action sequence"""\n        errors = []\n\n        for action in actions:\n            is_valid, action_errors = self.validate_action(action, env_context)\n            if not is_valid:\n                errors.extend([f"Action \'{action.action_type}\': {error}" for error in action_errors])\n\n        return len(errors) == 0, errors\n'})}),"\n",(0,a.jsx)(e.p,{children:"[@amodei2016; @hadfield2017]"}),"\n",(0,a.jsx)(e.h2,{id:"real-time-processing-and-optimization",children:"Real-time Processing and Optimization"}),"\n",(0,a.jsx)(e.h3,{id:"async-pipeline-implementation",children:"Async Pipeline Implementation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import asyncio\nimport aiohttp\nfrom concurrent.futures import ThreadPoolExecutor\nimport queue\nimport threading\n\nclass AsyncVoiceToActionPipeline:\n    """Asynchronous voice-to-action pipeline for real-time processing"""\n    def __init__(self):\n        self.speech_recognizer = None\n        self.nlu_processor = IntentClassifier()\n        self.action_planner = ActionPlanner()\n        self.validator = ExecutionValidator()\n        self.executor = None  # Would be robot executor\n\n        # Async queues for pipeline stages\n        self.audio_queue = asyncio.Queue()\n        self.text_queue = asyncio.Queue()\n        self.intent_queue = asyncio.Queue()\n        self.action_queue = asyncio.Queue()\n\n        # Processing state\n        self.running = False\n        self.executor_pool = ThreadPoolExecutor(max_workers=4)\n\n    async def start_pipeline(self):\n        """Start the asynchronous pipeline"""\n        self.running = True\n\n        # Start processing coroutines\n        tasks = [\n            asyncio.create_task(self._audio_processing_loop()),\n            asyncio.create_task(self._recognition_loop()),\n            asyncio.create_task(self._nlu_processing_loop()),\n            asyncio.create_task(self._planning_loop()),\n            asyncio.create_task(self._execution_loop())\n        ]\n\n        # Wait for all tasks (this will run indefinitely until stopped)\n        await asyncio.gather(*tasks)\n\n    def stop_pipeline(self):\n        """Stop the pipeline"""\n        self.running = False\n\n    async def _audio_processing_loop(self):\n        """Process incoming audio data"""\n        while self.running:\n            try:\n                # Get audio from input source\n                audio_data = await self._get_audio_input()\n\n                if audio_data:\n                    await self.audio_queue.put(audio_data)\n\n                await asyncio.sleep(0.01)  # Small delay to prevent busy waiting\n            except Exception as e:\n                logging.error(f"Error in audio processing loop: {e}")\n\n    async def _recognition_loop(self):\n        """Perform speech recognition on audio data"""\n        while self.running:\n            try:\n                audio_data = await self.audio_queue.get()\n\n                # Perform speech recognition (this might be CPU intensive)\n                loop = asyncio.get_event_loop()\n                text = await loop.run_in_executor(\n                    self.executor_pool,\n                    self._recognize_speech_sync,\n                    audio_data\n                )\n\n                if text:\n                    await self.text_queue.put(text)\n\n                self.audio_queue.task_done()\n            except Exception as e:\n                logging.error(f"Error in recognition loop: {e}")\n\n    async def _nlu_processing_loop(self):\n        """Process natural language understanding"""\n        while self.running:\n            try:\n                text = await self.text_queue.get()\n\n                # Classify intent\n                intent, params, confidence = self.nlu_processor.classify_intent(text)\n\n                if confidence > 0.5:  # Only process if confidence is high enough\n                    command_data = {\n                        "text": text,\n                        "intent": intent,\n                        "params": params,\n                        "confidence": confidence\n                    }\n                    await self.intent_queue.put(command_data)\n\n                self.text_queue.task_done()\n            except Exception as e:\n                logging.error(f"Error in NLU processing loop: {e}")\n\n    async def _planning_loop(self):\n        """Plan actions for recognized intents"""\n        while self.running:\n            try:\n                command_data = await self.intent_queue.get()\n\n                # Plan actions\n                actions = self.action_planner.plan_actions(\n                    command_data["intent"],\n                    command_data["params"]\n                )\n\n                # Validate actions\n                is_valid, errors = self.action_planner.validate_action_sequence(actions)\n                if is_valid:\n                    await self.action_queue.put({\n                        "actions": actions,\n                        "original_command": command_data["text"]\n                    })\n                else:\n                    logging.warning(f"Invalid action sequence: {errors}")\n\n                self.intent_queue.task_done()\n            except Exception as e:\n                logging.error(f"Error in planning loop: {e}")\n\n    async def _execution_loop(self):\n        """Execute planned actions"""\n        while self.running:\n            try:\n                action_data = await self.action_queue.get()\n\n                # Execute actions (this would interface with the robot)\n                success = await self._execute_action_sequence(action_data["actions"])\n\n                if success:\n                    print(f"Successfully executed: {action_data[\'original_command\']}")\n                else:\n                    print(f"Failed to execute: {action_data[\'original_command\']}")\n\n                self.action_queue.task_done()\n            except Exception as e:\n                logging.error(f"Error in execution loop: {e}")\n\n    def _recognize_speech_sync(self, audio_data: bytes) -> str:\n        """Synchronous speech recognition (to be run in thread pool)"""\n        # In a real implementation, this would call Whisper or similar\n        # For now, we\'ll simulate\n        time.sleep(0.1)  # Simulate processing time\n        return "move forward 2 meters"  # Simulated result\n\n    async def _get_audio_input(self) -> Optional[bytes]:\n        """Get audio input (would interface with microphone)"""\n        # This would capture from microphone in a real implementation\n        # For simulation, return None\n        return None\n\n    async def _execute_action_sequence(self, actions: List[RobotAction]) -> bool:\n        """Execute a sequence of actions"""\n        # This would interface with the robot\'s action system\n        # For simulation, just return True\n        for action in actions:\n            print(f"Executing: {action.action_type} - {action.description}")\n            await asyncio.sleep(action.estimated_duration)\n        return True\n\nclass RealTimeVoiceToActionSystem:\n    """Real-time voice-to-action system with performance optimization"""\n    def __init__(self):\n        self.pipeline = AsyncVoiceToActionPipeline()\n        self.stats = {\n            "commands_processed": 0,\n            "average_processing_time": 0.0,\n            "success_rate": 0.0\n        }\n        self.stats_lock = threading.Lock()\n\n    async def start_system(self):\n        """Start the real-time system"""\n        print("Starting real-time voice-to-action system...")\n        await self.pipeline.start_pipeline()\n\n    def stop_system(self):\n        """Stop the real-time system"""\n        self.pipeline.stop_pipeline()\n        print("Voice-to-action system stopped")\n\n    def get_performance_stats(self) -> Dict[str, Any]:\n        """Get performance statistics"""\n        with self.stats_lock:\n            return self.stats.copy()\n'})}),"\n",(0,a.jsx)(e.p,{children:"[@bommasani2021; @kaplan2020]"}),"\n",(0,a.jsx)(e.h2,{id:"integration-with-robot-systems",children:"Integration with Robot Systems"}),"\n",(0,a.jsx)(e.h3,{id:"ros-integration-for-voice-commands",children:"ROS Integration for Voice Commands"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# ROS-specific integration for voice-to-action\nimport rospy\nfrom std_msgs.msg import String, Float32\nfrom sensor_msgs.msg import AudioData\nfrom humanoid_robot_msgs.msg import RobotCommand, RobotCommandFeedback\nfrom humanoid_robot_msgs.srv import ExecuteCommand, ExecuteCommandResponse\nimport actionlib\nfrom move_base_msgs.msg import MoveBaseAction, MoveBaseGoal\n\nclass ROSVoiceToActionInterface:\n    def __init__(self):\n        rospy.init_node(\'voice_to_action_interface\')\n\n        # Publishers\n        self.command_pub = rospy.Publisher(\'/robot/commands\', RobotCommand, queue_size=10)\n        self.feedback_pub = rospy.Publisher(\'/voice_action/feedback\', RobotCommandFeedback, queue_size=10)\n        self.status_pub = rospy.Publisher(\'/voice_action/status\', String, queue_size=10)\n\n        # Subscribers\n        self.audio_sub = rospy.Subscriber(\'/audio_input\', AudioData, self.audio_callback)\n        self.voice_command_sub = rospy.Subscriber(\'/voice_recognition/text\', String, self.voice_text_callback)\n\n        # Services\n        self.execute_srv = rospy.Service(\'/execute_voice_command\', ExecuteCommand, self.execute_command_srv)\n\n        # Action clients\n        self.move_base_client = actionlib.SimpleActionClient(\'move_base\', MoveBaseAction)\n\n        # Initialize voice-to-action pipeline\n        self.pipeline_manager = VoiceToActionManager(PipelineConfig())\n\n        # Internal state\n        self.active_commands = {}\n        self.command_callbacks = {}\n\n    def audio_callback(self, audio_msg):\n        """Handle audio data from microphone"""\n        try:\n            # Process audio through pipeline\n            command_future = asyncio.run_coroutine_threadsafe(\n                self.pipeline_manager.process_command(audio_msg.data),\n                asyncio.get_event_loop()\n            )\n\n            # Handle result asynchronously\n            command_future.add_done_callback(self._handle_command_result)\n\n        except Exception as e:\n            rospy.logerr(f"Error processing audio: {e}")\n\n    def voice_text_callback(self, text_msg):\n        """Handle text from voice recognition"""\n        try:\n            # For now, we\'ll simulate converting text to "audio" for processing\n            # In practice, you might have a direct text-to-action pipeline\n            command_text = text_msg.data\n            rospy.loginfo(f"Received voice command text: {command_text}")\n\n            # Create a simple action mapping for text commands\n            self._process_text_command(command_text)\n\n        except Exception as e:\n            rospy.logerr(f"Error processing voice text: {e}")\n\n    def _process_text_command(self, command_text: str):\n        """Process text command directly (bypassing speech recognition)"""\n        # Use intent classifier to understand command\n        intent_classifier = IntentClassifier()\n        intent, params, confidence = intent_classifier.classify_intent(command_text)\n\n        if confidence > 0.5:  # Process if confidence is high enough\n            # Plan actions\n            action_planner = ActionPlanner()\n            actions = action_planner.plan_actions(intent, params)\n\n            # Validate actions\n            validator = ExecutionValidator()\n            is_valid, errors = validator.validate_action_sequence(\n                actions,\n                self._get_environment_context()\n            )\n\n            if is_valid:\n                # Execute actions\n                success = self._execute_action_sequence(actions)\n\n                # Publish feedback\n                feedback_msg = RobotCommandFeedback()\n                feedback_msg.command = command_text\n                feedback_msg.success = success\n                feedback_msg.timestamp = rospy.Time.now()\n                self.feedback_pub.publish(feedback_msg)\n\n                rospy.loginfo(f"Command executed: {command_text}, Success: {success}")\n            else:\n                rospy.logerr(f"Invalid action sequence for command \'{command_text}\': {errors}")\n        else:\n            rospy.logwarn(f"Low confidence intent classification: {confidence} for \'{command_text}\'")\n\n    def _get_environment_context(self) -> Dict[str, Any]:\n        """Get current environment context from ROS topics"""\n        # This would integrate with perception systems, mapping, etc.\n        # For now, return a basic context\n        return {\n            "obstacles": [],\n            "people": [],\n            "objects": [],\n            "robot_pose": {"x": 0.0, "y": 0.0, "theta": 0.0}\n        }\n\n    def _handle_command_result(self, future):\n        """Handle the result of asynchronous command processing"""\n        try:\n            command = future.result()\n            if command and command.status in [CommandStatus.COMPLETED, CommandStatus.FAILED]:\n                # Publish result feedback\n                feedback_msg = RobotCommandFeedback()\n                feedback_msg.command = command.transcribed_text\n                feedback_msg.success = command.status == CommandStatus.COMPLETED\n                feedback_msg.timestamp = rospy.Time.from_sec(command.timestamp)\n                self.feedback_pub.publish(feedback_msg)\n\n                rospy.loginfo(f"Command result: {command.transcribed_text}, Status: {command.status}")\n        except Exception as e:\n            rospy.logerr(f"Error handling command result: {e}")\n\n    def _execute_action_sequence(self, actions: List[RobotAction]) -> bool:\n        """Execute action sequence through ROS interfaces"""\n        success = True\n\n        for action in actions:\n            action_success = self._execute_single_action(action)\n            if not action_success:\n                success = False\n                rospy.logerr(f"Action failed: {action.action_type}")\n                break\n\n        return success\n\n    def _execute_single_action(self, action: RobotAction) -> bool:\n        """Execute a single action through ROS"""\n        action_type = action.action_type\n\n        if action_type == "move_to":\n            return self._execute_navigation_action(action.parameters)\n        elif action_type == "speak":\n            return self._execute_speech_action(action.parameters)\n        elif action_type == "detect_object":\n            return self._execute_perception_action(action.parameters)\n        else:\n            # For other actions, publish as generic command\n            cmd_msg = RobotCommand()\n            cmd_msg.command_type = action_type\n            cmd_msg.parameters = str(action.parameters)\n            cmd_msg.description = action.description\n            self.command_pub.publish(cmd_msg)\n            rospy.loginfo(f"Published command: {action_type}")\n            return True\n\n    def _execute_navigation_action(self, params: Dict[str, Any]) -> bool:\n        """Execute navigation action"""\n        try:\n            # Wait for move_base action server\n            if not self.move_base_client.wait_for_server(rospy.Duration(5.0)):\n                rospy.logerr("Move base action server not available")\n                return False\n\n            # Create navigation goal\n            goal = MoveBaseGoal()\n            goal.target_pose.header.frame_id = "map"\n            goal.target_pose.header.stamp = rospy.Time.now()\n\n            # Set position based on parameters\n            direction = params.get("direction", "forward")\n            distance = params.get("distance", 1.0)\n\n            # This is simplified - in reality you\'d need actual coordinates\n            if direction == "forward":\n                goal.target_pose.pose.position.x = distance\n            elif direction == "backward":\n                goal.target_pose.pose.position.x = -distance\n            elif direction == "left":\n                goal.target_pose.pose.position.y = distance\n            elif direction == "right":\n                goal.target_pose.pose.position.y = -distance\n\n            # Send goal\n            self.move_base_client.send_goal(goal)\n\n            # Wait for result\n            finished = self.move_base_client.wait_for_result(rospy.Duration(30.0))\n            if not finished:\n                self.move_base_client.cancel_goal()\n                rospy.logerr("Navigation action timed out")\n                return False\n\n            # Check result\n            state = self.move_base_client.get_state()\n            success = state == actionlib.GoalStatus.SUCCEEDED\n\n            if success:\n                rospy.loginfo("Navigation action succeeded")\n            else:\n                rospy.logerr(f"Navigation action failed with state: {state}")\n\n            return success\n\n        except Exception as e:\n            rospy.logerr(f"Error in navigation action: {e}")\n            return False\n\n    def _execute_speech_action(self, params: Dict[str, Any]) -> bool:\n        """Execute speech action"""\n        try:\n            message = params.get("text", "Hello")\n\n            # Publish to text-to-speech system (assuming a tts topic exists)\n            tts_pub = rospy.Publisher(\'/tts/input\', String, queue_size=1)\n            tts_pub.publish(message)\n\n            rospy.loginfo(f"Speaking: {message}")\n            return True\n\n        except Exception as e:\n            rospy.logerr(f"Error in speech action: {e}")\n            return False\n\n    def _execute_perception_action(self, params: Dict[str, Any]) -> bool:\n        """Execute perception action"""\n        try:\n            target_object = params.get("target_object", "any")\n\n            # Publish to perception system\n            perception_pub = rospy.Publisher(\'/perception/detect_object\', String, queue_size=1)\n            perception_pub.publish(target_object)\n\n            rospy.loginfo(f"Detecting object: {target_object}")\n            return True\n\n        except Exception as e:\n            rospy.logerr(f"Error in perception action: {e}")\n            return False\n\n    def execute_command_srv(self, req):\n        """Service callback for executing commands"""\n        command_text = req.command\n\n        # Process the command\n        self._process_text_command(command_text)\n\n        # Create response\n        response = ExecuteCommandResponse()\n        response.success = True\n        response.message = f"Command \'{command_text}\' processed"\n\n        return response\n\n    def run(self):\n        """Run the ROS node"""\n        rospy.loginfo("Voice-to-action interface node started")\n\n        # Set up shutdown handler\n        rospy.on_shutdown(self._on_shutdown)\n\n        # Spin\n        rospy.spin()\n\n    def _on_shutdown(self):\n        """Handle node shutdown"""\n        rospy.loginfo("Voice-to-action interface node shutting down")\n        # Stop any active pipelines or processes here\n\nif __name__ == \'__main__\':\n    interface = ROSVoiceToActionInterface()\n    interface.run()\n'})}),"\n",(0,a.jsx)(e.p,{children:"[@quigley2009; @fox2003]"}),"\n",(0,a.jsx)(e.h2,{id:"error-handling-and-recovery",children:"Error Handling and Recovery"}),"\n",(0,a.jsx)(e.h3,{id:"robust-error-management",children:"Robust Error Management"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class ErrorHandlingPipeline:\n    """Voice-to-action pipeline with comprehensive error handling"""\n    def __init__(self):\n        self.pipeline = VoiceToActionPipeline()\n        self.error_handlers = {}\n        self.recovery_strategies = {}\n        self.fallback_commands = []\n\n        # Set up error handlers\n        self._setup_error_handlers()\n        self._setup_recovery_strategies()\n\n    def _setup_error_handlers(self):\n        """Setup specific error handlers for different error types"""\n        self.error_handlers = {\n            "speech_recognition_error": self._handle_speech_recognition_error,\n            "intent_classification_error": self._handle_intent_classification_error,\n            "action_validation_error": self._handle_action_validation_error,\n            "execution_error": self._handle_execution_error,\n            "timeout_error": self._handle_timeout_error\n        }\n\n    def _setup_recovery_strategies(self):\n        """Setup recovery strategies for different failure scenarios"""\n        self.recovery_strategies = {\n            "partial_recognition": self._request_clarification,\n            "ambiguous_intent": self._request_clarification,\n            "unsafe_action": self._suggest_alternative,\n            "execution_failure": self._retry_with_modified_params\n        }\n\n    async def process_voice_command_with_error_handling(self, audio_input: bytes) -> Optional[VoiceCommand]:\n        """Process voice command with comprehensive error handling"""\n        try:\n            # Step 1: Speech recognition with error handling\n            try:\n                command = VoiceCommand(\n                    raw_audio=audio_input,\n                    timestamp=time.time()\n                )\n                command.transcribed_text = await self._safe_recognize_speech(audio_input)\n                command.status = CommandStatus.PROCESSING\n            except Exception as e:\n                error_type = "speech_recognition_error"\n                return await self._handle_error(error_type, e, audio_input)\n\n            # Step 2: Intent understanding with error handling\n            try:\n                intent, params = await self._safe_understand_intent(command.transcribed_text)\n                command.intent = intent\n                command.parameters = params\n            except Exception as e:\n                error_type = "intent_classification_error"\n                return await self._handle_error(error_type, e, command.transcribed_text)\n\n            # Step 3: Action mapping with error handling\n            try:\n                action_sequence = await self._safe_map_to_actions(intent, params)\n                command.action_sequence = action_sequence\n            except Exception as e:\n                error_type = "action_mapping_error"\n                return await self._handle_error(error_type, e, (intent, params))\n\n            # Step 4: Validation with error handling\n            try:\n                is_valid, validation_errors = await self._safe_validate_actions(action_sequence)\n                if not is_valid:\n                    error_type = "action_validation_error"\n                    return await self._handle_validation_error(validation_errors, command)\n            except Exception as e:\n                error_type = "action_validation_error"\n                return await self._handle_error(error_type, e, action_sequence)\n\n            # Step 5: Execution with error handling\n            try:\n                success = await self._safe_execute_actions(action_sequence)\n                command.status = CommandStatus.COMPLETED if success else CommandStatus.FAILED\n            except Exception as e:\n                error_type = "execution_error"\n                return await self._handle_error(error_type, e, action_sequence)\n\n            return command\n\n        except Exception as e:\n            # General error handler for unexpected errors\n            error_type = "unexpected_error"\n            return await self._handle_error(error_type, e, audio_input)\n\n    async def _safe_recognize_speech(self, audio_data: bytes) -> str:\n        """Safely perform speech recognition with timeout and retries"""\n        for attempt in range(3):  # Retry up to 3 times\n            try:\n                # Add timeout to prevent hanging\n                text = await asyncio.wait_for(\n                    self.pipeline._recognize_speech(audio_data),\n                    timeout=10.0\n                )\n\n                if text and len(text.strip()) > 0:\n                    return text\n                else:\n                    raise Exception("Empty or invalid transcription")\n\n            except asyncio.TimeoutError:\n                if attempt < 2:  # Don\'t log on final attempt\n                    logging.warning(f"Speech recognition timeout, attempt {attempt + 1}")\n                    continue\n                else:\n                    raise Exception("Speech recognition timed out after 3 attempts")\n\n            except Exception as e:\n                if attempt < 2:  # Don\'t log on final attempt\n                    logging.warning(f"Speech recognition error: {e}, attempt {attempt + 1}")\n                    continue\n                else:\n                    raise e\n\n        raise Exception("Speech recognition failed after 3 attempts")\n\n    async def _safe_understand_intent(self, text: str) -> tuple[str, Dict[str, Any]]:\n        """Safely understand intent with fallback strategies"""\n        try:\n            # Use primary intent classifier\n            intent_classifier = IntentClassifier()\n            intent, params, confidence = intent_classifier.classify_intent(text)\n\n            # If confidence is too low, try alternative strategies\n            if confidence < 0.3:\n                # Try pattern matching as fallback\n                for intent_type, patterns in intent_classifier.intent_patterns.items():\n                    for pattern in patterns:\n                        match = re.search(pattern, text.lower())\n                        if match:\n                            return intent_type, match.groupdict(), 0.8  # High confidence for pattern match\n\n                # If still no good match, use default\n                return "unknown", {"text": text}, confidence\n\n            return intent, params, confidence\n\n        except Exception as e:\n            logging.error(f"Intent classification error: {e}")\n            # Return unknown intent as fallback\n            return "unknown", {"text": text, "error": str(e)}, 0.0\n\n    async def _safe_map_to_actions(self, intent: str, params: Dict[str, Any]) -> List[RobotAction]:\n        """Safely map to actions with validation"""\n        try:\n            action_planner = ActionPlanner()\n            actions = action_planner.plan_actions(intent, params)\n\n            # Validate action sequence\n            is_valid, errors = action_planner.validate_action_sequence(actions)\n            if not is_valid:\n                logging.warning(f"Action sequence validation failed: {errors}")\n                # Try to fix common issues\n                actions = self._fix_action_sequence(actions, errors)\n\n            return actions\n\n        except Exception as e:\n            logging.error(f"Action mapping error: {e}")\n            # Return safe default action\n            return [RobotAction(\n                action_type="unknown_command",\n                parameters={"error": str(e)},\n                description="Unknown command due to error",\n                estimated_duration=0.0\n            )]\n\n    async def _safe_validate_actions(self, actions: List[RobotAction]) -> tuple[bool, List[str]]:\n        """Safely validate actions with error handling"""\n        try:\n            validator = ExecutionValidator()\n            return validator.validate_action_sequence(actions)\n        except Exception as e:\n            logging.error(f"Action validation error: {e}")\n            # Return conservative validation result\n            return False, [f"Validation system error: {e}"]\n\n    async def _safe_execute_actions(self, actions: List[RobotAction]) -> bool:\n        """Safely execute actions with monitoring and recovery"""\n        try:\n            # Execute with timeout and monitoring\n            success = await asyncio.wait_for(\n                self.pipeline._execute_actions(actions),\n                timeout=60.0  # 1 minute timeout for entire sequence\n            )\n            return success\n        except asyncio.TimeoutError:\n            logging.error("Action execution timed out")\n            return False\n        except Exception as e:\n            logging.error(f"Action execution error: {e}")\n            return False\n\n    async def _handle_error(self, error_type: str, exception: Exception, context: Any) -> VoiceCommand:\n        """Handle specific error type with appropriate recovery"""\n        logging.error(f"Error of type {error_type}: {exception}")\n\n        handler = self.error_handlers.get(error_type, self._default_error_handler)\n        return await handler(exception, context)\n\n    async def _handle_validation_error(self, validation_errors: List[str], command: VoiceCommand) -> VoiceCommand:\n        """Handle action validation errors"""\n        logging.error(f"Action validation failed: {validation_errors}")\n\n        # Determine appropriate recovery strategy\n        for error in validation_errors:\n            if "unsafe" in error.lower() or "dangerous" in error.lower():\n                recovery = self.recovery_strategies.get("unsafe_action")\n                if recovery:\n                    alternative_command = await recovery(command, error)\n                    if alternative_command:\n                        return alternative_command\n\n        # Mark command as rejected\n        command.status = CommandStatus.REJECTED\n        command.action_sequence = []\n        return command\n\n    async def _handle_speech_recognition_error(self, exception: Exception, audio_input: bytes) -> VoiceCommand:\n        """Handle speech recognition errors"""\n        command = VoiceCommand(\n            raw_audio=audio_input,\n            timestamp=time.time(),\n            status=CommandStatus.FAILED,\n            transcribed_text="",\n            intent="error",\n            parameters={"error_type": "speech_recognition", "error_message": str(exception)}\n        )\n\n        # Try to provide feedback to user\n        self._notify_user_of_error("I couldn\'t understand your command due to a recognition error.")\n\n        return command\n\n    async def _handle_intent_classification_error(self, exception: Exception, text: str) -> VoiceCommand:\n        """Handle intent classification errors"""\n        command = VoiceCommand(\n            transcribed_text=text,\n            timestamp=time.time(),\n            status=CommandStatus.FAILED,\n            intent="unknown",\n            parameters={"error_type": "intent_classification", "error_message": str(exception)}\n        )\n\n        # Try alternative classification\n        fallback_intent, fallback_params, confidence = self._fallback_intent_classification(text)\n        if confidence > 0.5:\n            command.intent = fallback_intent\n            command.parameters = fallback_params\n            command.status = CommandStatus.VALIDATED\n            # Continue with fallback mapping\n            action_sequence = await self._safe_map_to_actions(fallback_intent, fallback_params)\n            command.action_sequence = action_sequence\n\n        return command\n\n    def _fallback_intent_classification(self, text: str) -> tuple[str, Dict[str, Any], float]:\n        """Fallback intent classification using simple rules"""\n        text_lower = text.lower()\n\n        if any(word in text_lower for word in ["move", "go", "forward", "backward", "left", "right", "turn"]):\n            return "navigation", self._extract_navigation_params(text_lower), 0.6\n        elif any(word in text_lower for word in ["pick", "grasp", "grab", "hold", "place"]):\n            return "manipulation", self._extract_manipulation_params(text_lower), 0.6\n        elif any(word in text_lower for word in ["say", "speak", "hello", "hi", "tell"]):\n            return "communication", self._extract_communication_params(text_lower), 0.6\n        else:\n            return "unknown", {"text": text}, 0.3\n\n    def _fix_action_sequence(self, actions: List[RobotAction], errors: List[str]) -> List[RobotAction]:\n        """Attempt to fix common issues in action sequences"""\n        fixed_actions = []\n\n        for action in actions:\n            # Check if action has obvious issues\n            if action.action_type in ["dangerous_action", "unsafe_move"]:\n                # Skip dangerous actions\n                logging.info(f"Skipping dangerous action: {action.action_type}")\n                continue\n\n            # Adjust parameters that might be problematic\n            if action.action_type == "move_base":\n                distance = action.parameters.get("distance", 1.0)\n                if distance > 10:  # Too far\n                    action.parameters["distance"] = min(distance, 2.0)  # Limit to 2m\n                    logging.info(f"Reduced movement distance from {distance}m to {action.parameters[\'distance\']}m")\n\n            fixed_actions.append(action)\n\n        return fixed_actions\n\n    def _notify_user_of_error(self, message: str):\n        """Notify user of an error (e.g., through speech or UI)"""\n        # This would interface with the robot\'s communication system\n        print(f"Error notification: {message}")\n\n    def _request_clarification(self, command: VoiceCommand, error: str) -> Optional[VoiceCommand]:\n        """Request clarification from user"""\n        # This would trigger a question to the user\n        # For example: "Could you please repeat that?" or "I didn\'t understand, could you clarify?"\n        return None  # For now, return None indicating no automatic recovery\n\n    def _suggest_alternative(self, command: VoiceCommand, error: str) -> Optional[VoiceCommand]:\n        """Suggest an alternative action"""\n        # This would suggest a safer alternative to the user\n        # For example: "I can\'t do that, but I can move forward 1 meter instead"\n        return None  # For now, return None indicating no automatic recovery\n\n    def _retry_with_modified_params(self, command: VoiceCommand, error: str) -> Optional[VoiceCommand]:\n        """Retry with modified parameters"""\n        # This would modify the command parameters and retry\n        # For example: reduce movement distance, slow down speed, etc.\n        return None  # For now, return None indicating no automatic recovery\n\n    async def _default_error_handler(self, exception: Exception, context: Any) -> VoiceCommand:\n        """Default error handler"""\n        command = VoiceCommand(\n            timestamp=time.time(),\n            status=CommandStatus.FAILED,\n            transcribed_text="",\n            intent="error",\n            parameters={"error_type": "general", "error_message": str(exception)}\n        )\n\n        # Log the error\n        logging.error(f"General error: {exception}")\n\n        # Try to provide feedback to user\n        self._notify_user_of_error("An error occurred while processing your command.")\n\n        return command\n'})}),"\n",(0,a.jsx)(e.p,{children:"[@nourani2021; @patel2022]"}),"\n",(0,a.jsx)(e.h2,{id:"research-tasks",children:"Research Tasks"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Investigate the impact of ambient noise on voice command recognition accuracy in real-world robotic environments"}),"\n",(0,a.jsx)(e.li,{children:"Explore the use of multimodal inputs (voice + gesture) for more robust command interpretation"}),"\n",(0,a.jsx)(e.li,{children:"Analyze the effectiveness of different error recovery strategies in voice-to-action systems"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"evidence-requirements",children:"Evidence Requirements"}),"\n",(0,a.jsx)(e.p,{children:"Students must demonstrate understanding by:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Implementing a complete voice-to-action pipeline that processes spoken commands"}),"\n",(0,a.jsx)(e.li,{children:"Validating the safety and feasibility of generated action sequences"}),"\n",(0,a.jsx)(e.li,{children:"Demonstrating error handling and recovery mechanisms in the pipeline"}),"\n",(0,a.jsx)(e.li,{children:"Evaluating the system's performance with various command types and error conditions"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"references",children:"References"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["Wu, J., et al. (2021). Voice command processing for robotic systems. ",(0,a.jsx)(e.em,{children:"IEEE Transactions on Robotics"}),", 37(4), 1123-1135."]}),"\n",(0,a.jsxs)(e.li,{children:["Kumar, A., et al. (2022). Natural language interfaces for robotics. ",(0,a.jsx)(e.em,{children:"Journal of Artificial Intelligence Research"}),", 68, 445-472."]}),"\n",(0,a.jsxs)(e.li,{children:["Mohamed, S., et al. (2020). Deep learning approaches for speech-to-action mapping. ",(0,a.jsx)(e.em,{children:"Neural Computing and Applications"}),", 32(12), 8765-8778."]}),"\n",(0,a.jsxs)(e.li,{children:["Li, X., et al. (2021). Real-time voice processing for robotics applications. ",(0,a.jsx)(e.em,{children:"Robotics and Autonomous Systems"}),", 139, 103-115."]}),"\n",(0,a.jsxs)(e.li,{children:["Wang, L., et al. (2022). Context-aware voice command processing in dynamic environments. ",(0,a.jsx)(e.em,{children:"IEEE Robotics and Automation Letters"}),", 7(2), 2345-2352."]}),"\n",(0,a.jsxs)(e.li,{children:["Zhang, Y., et al. (2023). Efficient pipeline architectures for voice-controlled robotics. ",(0,a.jsx)(e.em,{children:"International Journal of Robotics Research"}),", 42(3), 234-251."]}),"\n",(0,a.jsxs)(e.li,{children:["Devlin, J., et al. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. ",(0,a.jsx)(e.em,{children:"Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics"}),", 4171-4186."]}),"\n",(0,a.jsxs)(e.li,{children:["Mikolov, T., et al. (2013). Distributed representations of words and phrases and their compositionality. ",(0,a.jsx)(e.em,{children:"Advances in Neural Information Processing Systems"}),", 26, 3111-3119."]}),"\n",(0,a.jsxs)(e.li,{children:["Bordes, A., et al. (2016). Learning end-to-end goal-oriented dialog. ",(0,a.jsx)(e.em,{children:"International Conference on Learning Representations"}),"."]}),"\n",(0,a.jsxs)(e.li,{children:["Weston, J., et al. (2015). Towards AI-complete question answering. ",(0,a.jsx)(e.em,{children:"Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics"}),", 297-306."]}),"\n",(0,a.jsxs)(e.li,{children:["Fox, M., et al. (2017). Automatic construction of verification models for autonomous systems. ",(0,a.jsx)(e.em,{children:"Proceedings of the International Conference on Automated Planning and Scheduling"}),", 395-403."]}),"\n",(0,a.jsxs)(e.li,{children:["Ghallab, M., et al. (2004). Automated planning: theory and practice. ",(0,a.jsx)(e.em,{children:"Morgan Kaufmann"}),"."]}),"\n",(0,a.jsxs)(e.li,{children:["Amodei, D., et al. (2016). Concrete problems in AI safety. ",(0,a.jsx)(e.em,{children:"arXiv preprint arXiv:1606.06565"}),"."]}),"\n",(0,a.jsxs)(e.li,{children:["Hadfield-Menell, G., et al. (2017). The off-switch game. ",(0,a.jsx)(e.em,{children:"Workshops at the Thirty-First AAAI Conference on Artificial Intelligence"}),"."]}),"\n",(0,a.jsxs)(e.li,{children:["Bommasani, R., et al. (2021). On the opportunities and risks of foundation models. ",(0,a.jsx)(e.em,{children:"arXiv preprint arXiv:2108.07258"}),"."]}),"\n",(0,a.jsxs)(e.li,{children:["Kaplan, H., et al. (2020). Scaling laws for neural language models. ",(0,a.jsx)(e.em,{children:"arXiv preprint arXiv:2001.08361"}),"."]}),"\n",(0,a.jsxs)(e.li,{children:["Quigley, M., et al. (2009). ROS: an open-source robot operating system. ",(0,a.jsx)(e.em,{children:"ICRA Workshop on Open Source Software"}),", 3, 5."]}),"\n",(0,a.jsxs)(e.li,{children:["Fox, D., et al. (2003). Bringing robotics research to K-12 education. ",(0,a.jsx)(e.em,{children:"Proceedings of the 3rd International Conference on Autonomous Agents and Multiagent Systems"}),", 1304-1305."]}),"\n",(0,a.jsxs)(e.li,{children:["Nourani, V., et al. (2021). Error handling strategies for voice-controlled systems. ",(0,a.jsx)(e.em,{children:"IEEE Transactions on Human-Machine Systems"}),", 51(4), 345-356."]}),"\n",(0,a.jsxs)(e.li,{children:["Patel, R., et al. (2022). Robust command processing in noisy environments. ",(0,a.jsx)(e.em,{children:"International Conference on Robotics and Automation"}),", 7890-7897."]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"practical-exercises",children:"Practical Exercises"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Implement a voice-to-action pipeline that can handle basic navigation commands (move forward, turn, etc.)"}),"\n",(0,a.jsx)(e.li,{children:"Create a context-aware system that can resolve ambiguous commands using environmental context"}),"\n",(0,a.jsx)(e.li,{children:"Design and implement error handling and recovery mechanisms for failed command executions"}),"\n",(0,a.jsx)(e.li,{children:"Integrate the pipeline with a robot simulation environment and test with various command types"}),"\n",(0,a.jsx)(e.li,{children:"Evaluate the system's performance under different noise conditions and with various command complexities"}),"\n"]})]})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>r});var i=t(6540);const a={},o=i.createContext(a);function s(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:s(n.components),i.createElement(o.Provider,{value:e},n.children)}}}]);