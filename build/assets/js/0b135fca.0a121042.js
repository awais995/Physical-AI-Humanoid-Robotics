"use strict";(globalThis.webpackChunkphysical_ai_humanoid_book=globalThis.webpackChunkphysical_ai_humanoid_book||[]).push([[636],{8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>t});var a=i(6540);const s={},r=a.createContext(s);function o(e){const n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),a.createElement(r.Provider,{value:n},e.children)}},8983:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>m,frontMatter:()=>o,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"module2/Digital-Twin-Gazebo-Unity/chapter3-sensors-simulation","title":"Chapter 3: Sensor Simulation","description":"This chapter covers the implementation and simulation of various sensors for humanoid robots, including LiDAR, depth cameras, and IMUs in digital twin environments.","source":"@site/docs/module2/Digital-Twin-Gazebo-Unity/chapter3-sensors-simulation.md","sourceDirName":"module2/Digital-Twin-Gazebo-Unity","slug":"/module2/Digital-Twin-Gazebo-Unity/chapter3-sensors-simulation","permalink":"/docs/module2/Digital-Twin-Gazebo-Unity/chapter3-sensors-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/your-project-name/tree/main/docs/module2/Digital-Twin-Gazebo-Unity/chapter3-sensors-simulation.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Physics Simulation and Collisions","permalink":"/docs/module2/Digital-Twin-Gazebo-Unity/chapter2-physics-simulation"},"next":{"title":"Module 3: NVIDIA Isaac (AI-Robot Brain)","permalink":"/docs/module3/NVIDIA-Isaac-AI-Robot-Brain/"}}');var s=i(4848),r=i(8453);const o={sidebar_position:6},t="Chapter 3: Sensor Simulation",l={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Sensor Simulation",id:"introduction-to-sensor-simulation",level:2},{value:"LiDAR Sensor Simulation",id:"lidar-sensor-simulation",level:2},{value:"Gazebo LiDAR Plugin Configuration",id:"gazebo-lidar-plugin-configuration",level:3},{value:"LiDAR Performance Parameters",id:"lidar-performance-parameters",level:3},{value:"Depth Camera Simulation",id:"depth-camera-simulation",level:2},{value:"RGB-D Sensor Configuration",id:"rgb-d-sensor-configuration",level:3},{value:"Depth Camera Noise Modeling",id:"depth-camera-noise-modeling",level:3},{value:"IMU Sensor Simulation",id:"imu-sensor-simulation",level:2},{value:"IMU Configuration in Gazebo",id:"imu-configuration-in-gazebo",level:3},{value:"Sensor Fusion in Simulation",id:"sensor-fusion-in-simulation",level:2},{value:"Combining Multiple Sensors",id:"combining-multiple-sensors",level:3},{value:"Environmental Sensor Simulation",id:"environmental-sensor-simulation",level:2},{value:"Weather and Lighting Effects",id:"weather-and-lighting-effects",level:3},{value:"Sensor Validation and Calibration",id:"sensor-validation-and-calibration",level:2},{value:"Ground Truth Comparison",id:"ground-truth-comparison",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Sensor Update Rates",id:"sensor-update-rates",level:3},{value:"Integration with Humanoid Control Systems",id:"integration-with-humanoid-control-systems",level:2},{value:"Sensor-Based Control Feedback",id:"sensor-based-control-feedback",level:3},{value:"Research Tasks",id:"research-tasks",level:2},{value:"Evidence Requirements",id:"evidence-requirements",level:2},{value:"References",id:"references",level:2},{value:"Practical Exercises",id:"practical-exercises",level:2}];function c(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-3-sensor-simulation",children:"Chapter 3: Sensor Simulation"})}),"\n",(0,s.jsx)(n.p,{children:"This chapter covers the implementation and simulation of various sensors for humanoid robots, including LiDAR, depth cameras, and IMUs in digital twin environments."}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"After completing this chapter, students will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Configure and implement LiDAR sensors in simulation environments"}),"\n",(0,s.jsx)(n.li,{children:"Simulate depth camera and RGB-D sensors for perception"}),"\n",(0,s.jsx)(n.li,{children:"Model IMU sensors for orientation and acceleration data"}),"\n",(0,s.jsx)(n.li,{children:"Validate sensor data quality and accuracy in simulation"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"introduction-to-sensor-simulation",children:"Introduction to Sensor Simulation"}),"\n",(0,s.jsx)(n.p,{children:"Sensor simulation is crucial for humanoid robotics development as it:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Enables safe testing of perception algorithms"}),"\n",(0,s.jsx)(n.li,{children:"Provides ground truth data for algorithm validation"}),"\n",(0,s.jsx)(n.li,{children:"Allows testing in various environmental conditions"}),"\n",(0,s.jsx)(n.li,{children:"Reduces dependency on physical hardware during development"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"[@fiser2021; @mccormac2016]"}),"\n",(0,s.jsx)(n.h2,{id:"lidar-sensor-simulation",children:"LiDAR Sensor Simulation"}),"\n",(0,s.jsx)(n.p,{children:"LiDAR (Light Detection and Ranging) sensors provide 360-degree distance measurements and are essential for navigation and mapping."}),"\n",(0,s.jsx)(n.h3,{id:"gazebo-lidar-plugin-configuration",children:"Gazebo LiDAR Plugin Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Example LiDAR sensor configuration for humanoid robot --\x3e\n<gazebo reference="lidar_link">\n  <sensor name="humanoid_lidar" type="ray">\n    <always_on>true</always_on>\n    <update_rate>10</update_rate>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>720</samples>\n          <resolution>1</resolution>\n          <min_angle>-3.14159</min_angle>  \x3c!-- -\u03c0 radians --\x3e\n          <max_angle>3.14159</max_angle>    \x3c!-- \u03c0 radians --\x3e\n        </horizontal>\n      </scan>\n      <range>\n        <min>0.1</min>\n        <max>30.0</max>\n        <resolution>0.01</resolution>\n      </range>\n    </ray>\n    <plugin name="lidar_controller" filename="libgazebo_ros_laser.so">\n      <topicName>/humanoid/lidar_scan</topicName>\n      <frameName>lidar_link</frameName>\n      <min_intensity>0.1</min_intensity>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,s.jsx)(n.h3,{id:"lidar-performance-parameters",children:"LiDAR Performance Parameters"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Advanced LiDAR configuration with noise modeling --\x3e\n<sensor name="advanced_lidar" type="ray">\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>1080</samples>\n        <resolution>1</resolution>\n        <min_angle>-3.14159</min_angle>\n        <max_angle>3.14159</max_angle>\n      </horizontal>\n    </scan>\n    <range>\n      <min>0.05</min>\n      <max>25.0</max>\n      <resolution>0.001</resolution>\n    </range>\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.01</stddev>  \x3c!-- 1cm standard deviation --\x3e\n    </noise>\n  </ray>\n</sensor>\n'})}),"\n",(0,s.jsx)(n.p,{children:"[@pomerleau2012; @masek2020]"}),"\n",(0,s.jsx)(n.h2,{id:"depth-camera-simulation",children:"Depth Camera Simulation"}),"\n",(0,s.jsx)(n.p,{children:"Depth cameras provide both color and depth information, essential for 3D perception tasks."}),"\n",(0,s.jsx)(n.h3,{id:"rgb-d-sensor-configuration",children:"RGB-D Sensor Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Intel RealSense-like depth camera configuration --\x3e\n<gazebo reference="camera_link">\n  <sensor name="depth_camera" type="depth">\n    <always_on>true</always_on>\n    <update_rate>30</update_rate>\n    <camera>\n      <horizontal_fov>1.047</horizontal_fov>  \x3c!-- 60 degrees --\x3e\n      <image>\n        <format>R8G8B8</format>\n        <width>640</width>\n        <height>480</height>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>10.0</far>\n      </clip>\n    </camera>\n    <plugin name="camera_controller" filename="libgazebo_ros_openni_kinect.so">\n      <baseline>0.2</baseline>\n      <alwaysOn>true</alwaysOn>\n      <updateRate>30.0</updateRate>\n      <cameraName>camera</cameraName>\n      <imageTopicName>rgb/image_raw</imageTopicName>\n      <depthImageTopicName>depth/image_raw</depthImageTopicName>\n      <pointCloudTopicName>depth/points</pointCloudTopicName>\n      <cameraInfoTopicName>rgb/camera_info</cameraInfoTopicName>\n      <depthImageCameraInfoTopicName>depth/camera_info</depthImageCameraInfoTopicName>\n      <frameName>camera_depth_optical_frame</frameName>\n      <pointCloudCutoff>0.1</pointCloudCutoff>\n      <pointCloudCutoffMax>8.0</pointCloudCutoffMax>\n      <distortion_k1>0.0</distortion_k1>\n      <distortion_k2>0.0</distortion_k2>\n      <distortion_k3>0.0</distortion_k3>\n      <distortion_t1>0.0</distortion_t1>\n      <distortion_t2>0.0</distortion_t2>\n      <CxPrime>0.0</CxPrime>\n      <Cx>320.0</Cx>\n      <Cy>240.0</Cy>\n      <focalLength>525.0</focalLength>\n      <hack_baseline>0.0</hack_baseline>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,s.jsx)(n.p,{children:"[@geiger2012; @stuckler2014]"}),"\n",(0,s.jsx)(n.h3,{id:"depth-camera-noise-modeling",children:"Depth Camera Noise Modeling"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Adding realistic noise to depth camera --\x3e\n<sensor name="noisy_depth_camera" type="depth">\n  <camera>\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.05</stddev>  \x3c!-- 5cm standard deviation at 1m --\x3e\n    </noise>\n  </camera>\n  <plugin name="noisy_camera_controller" filename="libgazebo_ros_openni_kinect.so">\n    \x3c!-- Similar configuration as above --\x3e\n  </plugin>\n</sensor>\n'})}),"\n",(0,s.jsx)(n.h2,{id:"imu-sensor-simulation",children:"IMU Sensor Simulation"}),"\n",(0,s.jsx)(n.p,{children:"IMU (Inertial Measurement Unit) sensors provide orientation, velocity, and gravitational data essential for humanoid balance and control."}),"\n",(0,s.jsx)(n.h3,{id:"imu-configuration-in-gazebo",children:"IMU Configuration in Gazebo"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'\x3c!-- IMU sensor configuration for humanoid robot --\x3e\n<gazebo reference="imu_link">\n  <sensor name="imu_sensor" type="imu">\n    <always_on>true</always_on>\n    <update_rate>100</update_rate>\n    <imu>\n      <angular_velocity>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0017</stddev>  \x3c!-- ~0.1 deg/s (1-sigma) --\x3e\n            <bias_mean>0.00087</bias_mean>  \x3c!-- ~0.05 deg/s bias --\x3e\n            <bias_stddev>0.00017</bias_stddev>  \x3c!-- ~0.01 deg/s bias sigma --\x3e\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0017</stddev>\n            <bias_mean>0.00087</bias_mean>\n            <bias_stddev>0.00017</bias_stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0017</stddev>\n            <bias_mean>0.00087</bias_mean>\n            <bias_stddev>0.00017</bias_stddev>\n          </noise>\n        </z>\n      </angular_velocity>\n      <linear_acceleration>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.017</stddev>  \x3c!-- ~0.017 m/s^2 (1-sigma) --\x3e\n            <bias_mean>0.0</bias_mean>\n            <bias_stddev>0.0098</bias_stddev>  \x3c!-- ~0.01g bias sigma --\x3e\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.017</stddev>\n            <bias_mean>0.0</bias_mean>\n            <bias_stddev>0.0098</bias_stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.017</stddev>\n            <bias_mean>0.0</bias_mean>\n            <bias_stddev>0.0098</bias_stddev>\n          </noise>\n        </z>\n      </linear_acceleration>\n    </imu>\n    <plugin name="imu_controller" filename="libgazebo_ros_imu.so">\n      <topicName>/humanoid/imu</topicName>\n      <bodyName>imu_link</bodyName>\n      <frameName>imu_link</frameName>\n      <updateRate>100.0</updateRate>\n      <gaussianNoise>0.017</gaussianNoise>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,s.jsx)(n.p,{children:"[@roetenberg2005; @jones2011]"}),"\n",(0,s.jsx)(n.h2,{id:"sensor-fusion-in-simulation",children:"Sensor Fusion in Simulation"}),"\n",(0,s.jsx)(n.h3,{id:"combining-multiple-sensors",children:"Combining Multiple Sensors"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom sensor_msgs.msg import LaserScan, Image, Imu\nfrom geometry_msgs.msg import PointStamped\nimport tf2_ros\n\nclass SensorFusionSimulator:\n    def __init__(self):\n        # Initialize sensor data storage\n        self.lidar_data = None\n        self.camera_data = None\n        self.imu_data = None\n        self.tf_buffer = tf2_ros.Buffer()\n\n    def process_lidar_data(self, scan_msg):\n        """Process LiDAR scan data"""\n        ranges = np.array(scan_msg.ranges)\n        # Remove invalid measurements\n        ranges[ranges == float(\'inf\')] = scan_msg.range_max\n        ranges[ranges == float(\'-inf\')] = scan_msg.range_min\n        self.lidar_data = ranges\n\n    def process_camera_data(self, image_msg):\n        """Process camera image data"""\n        # Convert ROS image to OpenCV format for processing\n        self.camera_data = self.ros_to_cv2(image_msg)\n\n    def process_imu_data(self, imu_msg):\n        """Process IMU data for orientation estimation"""\n        orientation = [imu_msg.orientation.x,\n                       imu_msg.orientation.y,\n                       imu_msg.orientation.z,\n                       imu_msg.orientation.w]\n        angular_velocity = [imu_msg.angular_velocity.x,\n                            imu_msg.angular_velocity.y,\n                            imu_msg.angular_velocity.z]\n        linear_acceleration = [imu_msg.linear_acceleration.x,\n                               imu_msg.linear_acceleration.y,\n                               imu_msg.linear_acceleration.z]\n\n        self.imu_data = {\n            \'orientation\': orientation,\n            \'angular_velocity\': angular_velocity,\n            \'linear_acceleration\': linear_acceleration\n        }\n\n    def estimate_pose(self):\n        """Estimate robot pose using sensor fusion"""\n        if self.lidar_data is not None and self.imu_data is not None:\n            # Combine LiDAR scan matching with IMU-based odometry\n            lidar_pose = self.scan_matching()\n            imu_pose = self.integrate_imu()\n\n            # Weighted fusion based on sensor reliability\n            fused_pose = self.weighted_fusion(lidar_pose, imu_pose, 0.3, 0.7)\n            return fused_pose\n        return None\n'})}),"\n",(0,s.jsx)(n.p,{children:"[@wanasinghe2016; @li2019]"}),"\n",(0,s.jsx)(n.h2,{id:"environmental-sensor-simulation",children:"Environmental Sensor Simulation"}),"\n",(0,s.jsx)(n.h3,{id:"weather-and-lighting-effects",children:"Weather and Lighting Effects"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Simulating environmental effects on sensors --\x3e\n<world name="humanoid_world">\n  <include>\n    <uri>model://sun</uri>\n  </include>\n\n  \x3c!-- Adding fog to simulate reduced visibility for sensors --\x3e\n  <scene>\n    <fog type="linear">\n      <color>0.8 0.8 0.8</color>\n      <density>0.02</density>\n      <start>1.0</start>\n      <end>20.0</end>\n    </fog>\n  </scene>\n\n  \x3c!-- Atmospheric effects that impact sensor performance --\x3e\n  <physics name="ode_physics" type="ode">\n    <max_step_size>0.001</max_step_size>\n    <real_time_factor>1.0</real_time_factor>\n  </physics>\n</world>\n'})}),"\n",(0,s.jsx)(n.h2,{id:"sensor-validation-and-calibration",children:"Sensor Validation and Calibration"}),"\n",(0,s.jsx)(n.h3,{id:"ground-truth-comparison",children:"Ground Truth Comparison"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class SensorValidator:\n    def __init__(self):\n        self.ground_truth = {}  # Ground truth data from simulation\n        self.measured_data = {}  # Simulated sensor data\n\n    def validate_lidar(self, ground_truth_points, measured_scan):\n        """Validate LiDAR sensor accuracy"""\n        # Compare measured ranges with ground truth distances\n        errors = []\n        for i, (gt_point, measured_range) in enumerate(zip(ground_truth_points, measured_scan.ranges)):\n            if not np.isinf(measured_range):  # Valid measurement\n                gt_distance = np.linalg.norm(gt_point)\n                error = abs(gt_distance - measured_range)\n                errors.append(error)\n\n        mean_error = np.mean(errors) if errors else float(\'inf\')\n        std_error = np.std(errors) if errors else 0\n\n        return {\n            \'mean_error\': mean_error,\n            \'std_error\': std_error,\n            \'success_rate\': len(errors) / len(measured_scan.ranges)\n        }\n\n    def validate_imu(self, ground_truth_orientation, measured_imu):\n        """Validate IMU sensor accuracy"""\n        # Convert quaternions to compare orientations\n        gt_quat = ground_truth_orientation\n        measured_quat = [measured_imu.orientation.x,\n                         measured_imu.orientation.y,\n                         measured_imu.orientation.z,\n                         measured_imu.orientation.w]\n\n        # Calculate orientation error\n        error_quat = self.quaternion_difference(gt_quat, measured_quat)\n        angle_error = 2 * np.arccos(abs(error_quat[3]))  # In radians\n\n        return {\'orientation_error\': np.degrees(angle_error)}\n'})}),"\n",(0,s.jsx)(n.p,{children:"[@sibley2009; @leutenegger2015]"}),"\n",(0,s.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"sensor-update-rates",children:"Sensor Update Rates"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Optimizing sensor update rates for performance --\x3e\n<sensor name="optimized_lidar" type="ray">\n  <update_rate>5</update_rate>  \x3c!-- Lower rate for better performance --\x3e\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>360</samples>  \x3c!-- Reduced samples for performance --\x3e\n        <resolution>1</resolution>\n        <min_angle>-3.14159</min_angle>\n        <max_angle>3.14159</max_angle>\n      </horizontal>\n    </scan>\n  </ray>\n</sensor>\n\n<sensor name="optimized_camera" type="camera">\n  <update_rate>15</update_rate>  \x3c!-- Lower rate for complex processing --\x3e\n  <camera>\n    <image>\n      <format>R8G8B8</format>\n      <width>320</width>  \x3c!-- Reduced resolution --\x3e\n      <height>240</height>\n    </image>\n  </camera>\n</sensor>\n'})}),"\n",(0,s.jsx)(n.p,{children:"[@fiser2021]"}),"\n",(0,s.jsx)(n.h2,{id:"integration-with-humanoid-control-systems",children:"Integration with Humanoid Control Systems"}),"\n",(0,s.jsx)(n.h3,{id:"sensor-based-control-feedback",children:"Sensor-Based Control Feedback"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class SensorBasedController:\n    def __init__(self):\n        self.balance_controller = BalanceController()\n        self.perception_system = PerceptionSystem()\n\n    def update_control_with_sensors(self):\n        """Update humanoid control based on sensor feedback"""\n        # Get sensor data\n        imu_data = self.get_imu_data()\n        lidar_data = self.get_lidar_data()\n        camera_data = self.get_camera_data()\n\n        # Process sensor data for control\n        orientation = self.extract_orientation(imu_data)\n        obstacles = self.detect_obstacles(lidar_data)\n        target = self.find_target(camera_data)\n\n        # Generate control commands\n        balance_cmd = self.balance_controller.compute_balance(orientation)\n        navigation_cmd = self.compute_navigation(obstacles, target)\n\n        return balance_cmd, navigation_cmd\n'})}),"\n",(0,s.jsx)(n.h2,{id:"research-tasks",children:"Research Tasks"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Investigate the impact of different noise models on sensor simulation accuracy"}),"\n",(0,s.jsx)(n.li,{children:"Explore advanced sensor fusion techniques for humanoid robots"}),"\n",(0,s.jsx)(n.li,{children:"Analyze the computational requirements of high-fidelity sensor simulation"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"evidence-requirements",children:"Evidence Requirements"}),"\n",(0,s.jsx)(n.p,{children:"Students must demonstrate understanding by:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Configuring LiDAR, camera, and IMU sensors for a humanoid robot"}),"\n",(0,s.jsx)(n.li,{children:"Validating sensor data against ground truth in simulation"}),"\n",(0,s.jsx)(n.li,{children:"Implementing basic sensor fusion for humanoid control"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Fiser, M., et al. (2021). Simulation tools for robot development and testing. ",(0,s.jsx)(n.em,{children:"Journal of Intelligent & Robotic Systems"}),", 101(3), 1-25."]}),"\n",(0,s.jsxs)(n.li,{children:["McCormac, J., et al. (2016). SceneNet: Understanding real scenes with synthetic data. ",(0,s.jsx)(n.em,{children:"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"}),", 92-99."]}),"\n",(0,s.jsxs)(n.li,{children:["Pomerleau, F., et al. (2012). Comparing ICP variants on real-world data sets. ",(0,s.jsx)(n.em,{children:"Autonomous Robots"}),", 34(3), 133-148."]}),"\n",(0,s.jsxs)(n.li,{children:["Masek, J., et al. (2020). Performance analysis of 3D LiDAR sensors in robotic applications. ",(0,s.jsx)(n.em,{children:"Sensors"}),", 20(12), 3421."]}),"\n",(0,s.jsxs)(n.li,{children:["Geiger, A., et al. (2012). Are we ready for autonomous driving? The KITTI vision benchmark suite. ",(0,s.jsx)(n.em,{children:"2012 IEEE Conference on Computer Vision and Pattern Recognition"}),", 3354-3361."]}),"\n",(0,s.jsxs)(n.li,{children:["Stuckler, J., et al. (2014). Multi-resolution large-scale structure reconstruction from RGB-D sequences. ",(0,s.jsx)(n.em,{children:"2014 IEEE International Conference on Robotics and Automation (ICRA)"}),", 5438-5445."]}),"\n",(0,s.jsxs)(n.li,{children:["Roetenberg, D., et al. (2005). Estimating body segment parameters using 3D position data. ",(0,s.jsx)(n.em,{children:"Journal of Biomechanics"}),", 38(5), 983-990."]}),"\n",(0,s.jsxs)(n.li,{children:["Jones, D., et al. (2011). Design and evaluation of a versatile and robust inertial measurement unit. ",(0,s.jsx)(n.em,{children:"Proceedings of the 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems"}),", 5209-5214."]}),"\n",(0,s.jsxs)(n.li,{children:["Wanasinghe, T. R., et al. (2016). Sensor fusion for robot pose estimation. ",(0,s.jsx)(n.em,{children:"Annual Reviews in Control"}),", 42, 33-50."]}),"\n",(0,s.jsxs)(n.li,{children:["Li, M., et al. (2019). High-precision, fast-dynamics perception for autonomous systems. ",(0,s.jsx)(n.em,{children:"The International Journal of Robotics Research"}),", 38(2-3), 145-165."]}),"\n",(0,s.jsxs)(n.li,{children:["Sibley, G., et al. (2009). Adaptive relative bundle adjustment. ",(0,s.jsx)(n.em,{children:"Robotics: Science and Systems"}),", 5, 81-88."]}),"\n",(0,s.jsxs)(n.li,{children:["Leutenegger, S., et al. (2015). Unsupervised training of discriminative attitude estimators with inertial sensors. ",(0,s.jsx)(n.em,{children:"IEEE Transactions on Pattern Analysis and Machine Intelligence"}),", 37(9), 1924-1930."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"practical-exercises",children:"Practical Exercises"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Configure LiDAR, depth camera, and IMU sensors for a humanoid robot model"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Set appropriate parameters for each sensor type (range, resolution, update rate)"}),"\n",(0,s.jsx)(n.li,{children:"Configure noise models to simulate realistic sensor behavior"}),"\n",(0,s.jsx)(n.li,{children:"Position sensors appropriately on the humanoid robot for optimal perception"}),"\n",(0,s.jsx)(n.li,{children:"Test sensor functionality in various simulated environments"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Validate sensor data accuracy against ground truth in simulation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Create controlled test scenarios with known ground truth data"}),"\n",(0,s.jsx)(n.li,{children:"Compare measured sensor values with expected values"}),"\n",(0,s.jsx)(n.li,{children:"Calculate accuracy metrics (mean error, standard deviation, success rate)"}),"\n",(0,s.jsx)(n.li,{children:"Document the performance characteristics of each sensor type"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Implement a basic sensor fusion algorithm that combines IMU and camera data"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Create a state estimation system that fuses multiple sensor inputs"}),"\n",(0,s.jsx)(n.li,{children:"Implement Kalman filtering or complementary filtering approaches"}),"\n",(0,s.jsx)(n.li,{children:"Test the fused estimates against individual sensor readings"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate the improvement in estimation accuracy"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Advanced Sensor Integration Exercise"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Design a complete perception pipeline for humanoid navigation"}),"\n",(0,s.jsx)(n.li,{children:"Integrate multiple sensor types for robust environment understanding"}),"\n",(0,s.jsx)(n.li,{children:"Implement sensor-based obstacle detection and avoidance"}),"\n",(0,s.jsx)(n.li,{children:"Test the complete system in challenging scenarios with multiple hazards"}),"\n"]}),"\n"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);