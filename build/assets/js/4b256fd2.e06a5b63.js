"use strict";(globalThis.webpackChunkphysical_ai_humanoid_book=globalThis.webpackChunkphysical_ai_humanoid_book||[]).push([[892],{1175:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>m,frontMatter:()=>a,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module4/Vision-Language-Action-VLA/chapter1-whisper-commands","title":"Chapter 1: Whisper Voice Commands","description":"This chapter covers implementing voice-activated robotics systems using OpenAI\'s Whisper for voice command recognition in humanoid robots.","source":"@site/docs/module4/Vision-Language-Action-VLA/chapter1-whisper-commands.md","sourceDirName":"module4/Vision-Language-Action-VLA","slug":"/module4/Vision-Language-Action-VLA/chapter1-whisper-commands","permalink":"/docs/module4/Vision-Language-Action-VLA/chapter1-whisper-commands","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/your-project-name/tree/main/docs/module4/Vision-Language-Action-VLA/chapter1-whisper-commands.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action (VLA) + Capstone","permalink":"/docs/module4/Vision-Language-Action-VLA/"},"next":{"title":"Chapter 2: LLM-based Cognitive Planning","permalink":"/docs/module4/Vision-Language-Action-VLA/chapter2-llm-planning"}}');var t=o(4848),s=o(8453);const a={sidebar_position:2},r="Chapter 1: Whisper Voice Commands",c={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Whisper for Robotics",id:"introduction-to-whisper-for-robotics",level:2},{value:"Whisper Architecture and Robotics Integration",id:"whisper-architecture-and-robotics-integration",level:2},{value:"Core Whisper Components",id:"core-whisper-components",level:3},{value:"Advanced Whisper Configurations for Robotics",id:"advanced-whisper-configurations-for-robotics",level:2},{value:"Fine-tuning Whisper for Robotics Commands",id:"fine-tuning-whisper-for-robotics-commands",level:3},{value:"Voice Command Processing Pipeline",id:"voice-command-processing-pipeline",level:2},{value:"Real-time Voice Command Pipeline",id:"real-time-voice-command-pipeline",level:3},{value:"Integration with ROS and Humanoid Control",id:"integration-with-ros-and-humanoid-control",level:2},{value:"ROS Integration for Voice Commands",id:"ros-integration-for-voice-commands",level:3},{value:"Voice Command Validation and Error Handling",id:"voice-command-validation-and-error-handling",level:2},{value:"Confidence-Based Command Validation",id:"confidence-based-command-validation",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Optimized Whisper Pipeline for Real-time Robotics",id:"optimized-whisper-pipeline-for-real-time-robotics",level:3},{value:"Research Tasks",id:"research-tasks",level:2},{value:"Evidence Requirements",id:"evidence-requirements",level:2},{value:"References",id:"references",level:2},{value:"Practical Exercises",id:"practical-exercises",level:2}];function l(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-1-whisper-voice-commands",children:"Chapter 1: Whisper Voice Commands"})}),"\n",(0,t.jsx)(n.p,{children:"This chapter covers implementing voice-activated robotics systems using OpenAI's Whisper for voice command recognition in humanoid robots."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"After completing this chapter, students will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Set up and configure Whisper for real-time voice command recognition"}),"\n",(0,t.jsx)(n.li,{children:"Process voice commands for humanoid robot control"}),"\n",(0,t.jsx)(n.li,{children:"Implement voice command grammars and vocabularies for robotics"}),"\n",(0,t.jsx)(n.li,{children:"Integrate Whisper with humanoid robot control systems"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction-to-whisper-for-robotics",children:"Introduction to Whisper for Robotics"}),"\n",(0,t.jsx)(n.p,{children:"Whisper is OpenAI's robust speech recognition model that excels at transcribing speech in various languages and conditions. For humanoid robotics, Whisper provides the foundation for natural language interaction, enabling users to control robots using voice commands."}),"\n",(0,t.jsx)(n.p,{children:"Whisper's key advantages for robotics applications:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"High accuracy across different accents and speaking styles"}),"\n",(0,t.jsx)(n.li,{children:"Robustness to background noise"}),"\n",(0,t.jsx)(n.li,{children:"Support for multiple languages"}),"\n",(0,t.jsx)(n.li,{children:"Efficient inference capabilities"}),"\n",(0,t.jsx)(n.li,{children:"Fine-tuning capabilities for domain-specific vocabulary"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"[@radford2022; @openai2022]"}),"\n",(0,t.jsx)(n.h2,{id:"whisper-architecture-and-robotics-integration",children:"Whisper Architecture and Robotics Integration"}),"\n",(0,t.jsx)(n.h3,{id:"core-whisper-components",children:"Core Whisper Components"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Example Whisper integration for humanoid robot voice commands\nimport whisper\nimport torch\nimport rospy\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nimport threading\nimport pyaudio\nimport wave\nimport time\n\nclass WhisperVoiceCommandProcessor:\n    def __init__(self, model_size=\"base\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n        self.model = whisper.load_model(model_size).to(device)\n        self.device = device\n        self.command_publisher = rospy.Publisher('/voice_commands', String, queue_size=10)\n        self.active = False\n\n        # Audio recording parameters\n        self.chunk = 1024\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        self.rate = 44100\n        self.record_seconds = 5\n        self.threshold = 500  # Minimum amplitude to trigger recording\n\n        # Robot command mappings\n        self.command_mappings = {\n            'move forward': 'move_forward',\n            'move backward': 'move_backward',\n            'turn left': 'turn_left',\n            'turn right': 'turn_right',\n            'stop': 'stop',\n            'raise arm': 'raise_arm',\n            'lower arm': 'lower_arm',\n            'pick up object': 'pick_up_object',\n            'put down object': 'put_down_object',\n            'walk': 'walk',\n            'stand': 'stand',\n            'sit': 'sit',\n            'look around': 'look_around',\n            'dance': 'dance',\n            'introduce yourself': 'introduce',\n            'what can you do': 'capabilities',\n            'help': 'help'\n        }\n\n    def start_listening(self):\n        \"\"\"Start continuous voice command listening\"\"\"\n        self.active = True\n        print(\"Starting voice command listening...\")\n\n        # Start audio recording thread\n        audio_thread = threading.Thread(target=self.record_audio_loop)\n        audio_thread.daemon = True\n        audio_thread.start()\n\n    def stop_listening(self):\n        \"\"\"Stop voice command listening\"\"\"\n        self.active = False\n        print(\"Stopped voice command listening\")\n\n    def record_audio_loop(self):\n        \"\"\"Continuously record audio and process voice commands\"\"\"\n        audio = pyaudio.PyAudio()\n\n        while self.active:\n            # Detect if there's speech (simple energy-based detection)\n            if self.is_speech_detected():\n                print(\"Recording voice command...\")\n                frames = []\n\n                # Record audio\n                stream = audio.open(format=self.format,\n                                  channels=self.channels,\n                                  rate=self.rate,\n                                  input=True,\n                                  frames_per_buffer=self.chunk)\n\n                for i in range(0, int(self.rate / self.chunk * self.record_seconds)):\n                    data = stream.read(self.chunk)\n                    frames.append(data)\n\n                    # Break if silence is detected\n                    if not self.is_speech_detected_in_buffer(data):\n                        break\n\n                stream.stop_stream()\n                stream.close()\n\n                # Save and process the audio\n                audio_file = self.save_audio_buffer(frames)\n                self.process_voice_command(audio_file)\n\n        audio.terminate()\n\n    def is_speech_detected(self):\n        \"\"\"Simple energy-based speech detection\"\"\"\n        # Initialize PyAudio\n        audio = pyaudio.PyAudio()\n\n        # Open stream for short sample\n        stream = audio.open(format=self.format,\n                          channels=self.channels,\n                          rate=self.rate,\n                          input=True,\n                          frames_per_buffer=self.chunk)\n\n        # Read a chunk of audio\n        data = stream.read(self.chunk, exception_on_overflow=False)\n        stream.stop_stream()\n        stream.close()\n        audio.terminate()\n\n        # Convert to numpy array and calculate energy\n        import numpy as np\n        audio_data = np.frombuffer(data, dtype=np.int16)\n        energy = np.sum(audio_data ** 2) / len(audio_data)\n\n        return energy > self.threshold\n\n    def is_speech_detected_in_buffer(self, buffer_data):\n        \"\"\"Check if buffer contains speech\"\"\"\n        import numpy as np\n        audio_data = np.frombuffer(buffer_data, dtype=np.int16)\n        energy = np.sum(audio_data ** 2) / len(audio_data)\n        return energy > (self.threshold / 2)  # Lower threshold for continuous speech\n\n    def save_audio_buffer(self, frames):\n        \"\"\"Save recorded frames to a temporary WAV file\"\"\"\n        import tempfile\n        import os\n\n        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')\n        temp_filename = temp_file.name\n        temp_file.close()\n\n        wf = wave.open(temp_filename, 'wb')\n        wf.setnchannels(self.channels)\n        wf.setsampwidth(pyaudio.PyAudio().get_sample_size(self.format))\n        wf.setframerate(self.rate)\n        wf.writeframes(b''.join(frames))\n        wf.close()\n\n        return temp_filename\n\n    def process_voice_command(self, audio_file):\n        \"\"\"Process audio file with Whisper and execute robot command\"\"\"\n        try:\n            # Transcribe the audio using Whisper\n            result = self.model.transcribe(audio_file)\n            transcribed_text = result['text'].strip().lower()\n\n            print(f\"Transcribed: {transcribed_text}\")\n\n            # Publish the raw transcription\n            self.command_publisher.publish(transcribed_text)\n\n            # Execute command if recognized\n            command = self.map_to_robot_command(transcribed_text)\n            if command:\n                print(f\"Executing command: {command}\")\n                self.execute_robot_command(command)\n            else:\n                print(\"Command not recognized\")\n\n        except Exception as e:\n            print(f\"Error processing voice command: {e}\")\n        finally:\n            # Clean up the temporary file\n            import os\n            if os.path.exists(audio_file):\n                os.remove(audio_file)\n\n    def map_to_robot_command(self, text):\n        \"\"\"Map transcribed text to robot command\"\"\"\n        # Check for direct command matches\n        for cmd_text, cmd_func in self.command_mappings.items():\n            if cmd_text in text:\n                return cmd_func\n\n        # Check for partial matches and synonyms\n        if any(word in text for word in ['forward', 'ahead', 'go']):\n            return 'move_forward'\n        elif any(word in text for word in ['backward', 'back', 'reverse']):\n            return 'move_backward'\n        elif any(word in text for word in ['left', 'turn left']):\n            return 'turn_left'\n        elif any(word in text for word in ['right', 'turn right']):\n            return 'turn_right'\n        elif 'stop' in text or 'halt' in text:\n            return 'stop'\n        elif any(word in text for word in ['raise', 'up', 'lift']):\n            return 'raise_arm'\n        elif any(word in text for word in ['lower', 'down', 'drop']):\n            return 'lower_arm'\n\n        return None\n\n    def execute_robot_command(self, command):\n        \"\"\"Execute the mapped robot command\"\"\"\n        # This would interface with the robot's action system\n        print(f\"Sending command to robot: {command}\")\n\n        # Example: Publish command to robot control system\n        # rospy.Publisher(f'/robot/{command}', Empty, queue_size=1).publish()\n\n        # For demonstration, just log the command\n        rospy.loginfo(f\"Executing robot command: {command}\")\n"})}),"\n",(0,t.jsx)(n.p,{children:"[@radford2022; @serdyukov2023]"}),"\n",(0,t.jsx)(n.h2,{id:"advanced-whisper-configurations-for-robotics",children:"Advanced Whisper Configurations for Robotics"}),"\n",(0,t.jsx)(n.h3,{id:"fine-tuning-whisper-for-robotics-commands",children:"Fine-tuning Whisper for Robotics Commands"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Example of fine-tuning Whisper for robotics-specific vocabulary\nimport whisper\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport json\n\nclass RoboticsCommandDataset(Dataset):\n    def __init__(self, data_path):\n        """\n        Dataset for robotics-specific voice commands\n        data_path: Path to JSON file with {\'audio_path\': \'path\', \'text\': \'transcription\'} entries\n        """\n        with open(data_path, \'r\') as f:\n            self.data = json.load(f)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        audio_path = item[\'audio_path\']\n        text = item[\'text\']\n\n        # Load audio file\n        audio = whisper.load_audio(audio_path)\n        audio = whisper.pad_or_trim(audio)\n\n        # Convert to log mel spectrogram\n        mel = whisper.log_mel_spectrogram(audio)\n\n        return mel, text\n\ndef fine_tune_whisper_for_robotics(base_model_name, train_dataset, output_path, epochs=3):\n    """Fine-tune Whisper for robotics-specific commands"""\n    # Load base model\n    model = whisper.load_model(base_model_name)\n\n    # Prepare training data\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n\n    # Move model to training mode\n    model.train()\n\n    # Define optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n    # Training loop\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch_idx, (mels, texts) in enumerate(train_loader):\n            optimizer.zero_grad()\n\n            # Encode audio\n            audio_features = model.encoder(mels)\n\n            # Prepare text tokens\n            text_tokens = [whisper.tokenize.TextTokenProcessor(text, model.dims.n_vocab) for text in texts]\n\n            # Decode text\n            out = model.decoder(text_tokens, audio_features)\n\n            # Calculate loss (simplified - actual implementation would require more complex loss)\n            # In practice, you\'d use the official OpenAI whisper training code\n            # This is a conceptual example\n\n            loss = calculate_training_loss(out, text_tokens)\n            loss.backward()\n\n            optimizer.step()\n            total_loss += loss.item()\n\n            if batch_idx % 10 == 0:\n                print(f\'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}\')\n\n    # Save fine-tuned model\n    torch.save(model.state_dict(), output_path)\n    print(f"Fine-tuned model saved to {output_path}")\n\ndef calculate_training_loss(outputs, targets):\n    """Calculate training loss for Whisper fine-tuning"""\n    # Simplified loss calculation - in practice this would be more complex\n    # using cross-entropy between predicted and target token sequences\n    pass\n'})}),"\n",(0,t.jsx)(n.p,{children:"[@zhu2021; @zhang2022]"}),"\n",(0,t.jsx)(n.h2,{id:"voice-command-processing-pipeline",children:"Voice Command Processing Pipeline"}),"\n",(0,t.jsx)(n.h3,{id:"real-time-voice-command-pipeline",children:"Real-time Voice Command Pipeline"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Real-time voice command processing pipeline for humanoid robots\nimport asyncio\nimport queue\nimport threading\nfrom dataclasses import dataclass\nfrom typing import Optional, Callable\nimport time\n\n@dataclass\nclass VoiceCommand:\n    text: str\n    confidence: float\n    timestamp: float\n    robot_command: Optional[str] = None\n\nclass VoiceCommandPipeline:\n    def __init__(self, whisper_model_size="base"):\n        self.whisper_processor = WhisperVoiceCommandProcessor(whisper_model_size)\n        self.command_queue = queue.Queue()\n        self.result_callbacks = []\n        self.active = False\n\n    def add_result_callback(self, callback: Callable[[VoiceCommand], None]):\n        """Add callback to be called when a voice command is processed"""\n        self.result_callbacks.append(callback)\n\n    def start_pipeline(self):\n        """Start the voice command processing pipeline"""\n        self.active = True\n\n        # Start Whisper processor\n        self.whisper_processor.start_listening()\n\n        # Start result processing thread\n        result_thread = threading.Thread(target=self.process_results)\n        result_thread.daemon = True\n        result_thread.start()\n\n    def stop_pipeline(self):\n        """Stop the voice command processing pipeline"""\n        self.active = False\n        self.whisper_processor.stop_listening()\n\n    def process_results(self):\n        """Process results from Whisper and execute callbacks"""\n        while self.active:\n            try:\n                # Wait for command from Whisper processor\n                command = self.command_queue.get(timeout=1)\n\n                # Execute all registered callbacks\n                for callback in self.result_callbacks:\n                    try:\n                        callback(command)\n                    except Exception as e:\n                        print(f"Error in result callback: {e}")\n\n            except queue.Empty:\n                continue\n\n    def process_command_result(self, text: str, confidence: float = 0.8):\n        """Process a command result from Whisper"""\n        command = VoiceCommand(\n            text=text,\n            confidence=confidence,\n            timestamp=time.time(),\n            robot_command=self.map_to_robot_command(text)\n        )\n\n        # Add to queue for processing\n        self.command_queue.put(command)\n\n        return command\n\nclass RoboticsCommandExecutor:\n    def __init__(self):\n        self.command_handlers = {\n            \'move_forward\': self.handle_move_forward,\n            \'move_backward\': self.handle_move_backward,\n            \'turn_left\': self.handle_turn_left,\n            \'turn_right\': self.handle_turn_right,\n            \'stop\': self.handle_stop,\n            \'raise_arm\': self.handle_raise_arm,\n            \'lower_arm\': self.handle_lower_arm,\n            \'pick_up_object\': self.handle_pick_up,\n            \'put_down_object\': self.handle_put_down,\n            \'walk\': self.handle_walk,\n            \'stand\': self.handle_stand,\n            \'sit\': self.handle_sit,\n            \'look_around\': self.handle_look_around,\n            \'dance\': self.handle_dance,\n            \'introduce\': self.handle_introduce,\n            \'capabilities\': self.handle_capabilities,\n            \'help\': self.handle_help\n        }\n\n    def execute_command(self, voice_command: VoiceCommand):\n        """Execute a voice command on the robot"""\n        if not voice_command.robot_command:\n            print(f"Command not recognized: {voice_command.text}")\n            return False\n\n        if voice_command.confidence < 0.5:  # Confidence threshold\n            print(f"Low confidence command ignored: {voice_command.text} (confidence: {voice_command.confidence})")\n            return False\n\n        handler = self.command_handlers.get(voice_command.robot_command)\n        if handler:\n            print(f"Executing: {voice_command.robot_command}")\n            handler()\n            return True\n        else:\n            print(f"No handler for command: {voice_command.robot_command}")\n            return False\n\n    # Command handler implementations\n    def handle_move_forward(self):\n        print("Moving robot forward")\n        # Implement actual robot movement\n\n    def handle_move_backward(self):\n        print("Moving robot backward")\n        # Implement actual robot movement\n\n    def handle_turn_left(self):\n        print("Turning robot left")\n        # Implement actual robot movement\n\n    def handle_turn_right(self):\n        print("Turning robot right")\n        # Implement actual robot movement\n\n    def handle_stop(self):\n        print("Stopping robot")\n        # Implement actual robot stop\n\n    def handle_raise_arm(self):\n        print("Raising robot arm")\n        # Implement actual arm movement\n\n    def handle_lower_arm(self):\n        print("Lowering robot arm")\n        # Implement actual arm movement\n\n    def handle_pick_up(self):\n        print("Picking up object")\n        # Implement actual pick-up action\n\n    def handle_put_down(self):\n        print("Putting down object")\n        # Implement actual put-down action\n\n    def handle_walk(self):\n        print("Robot walking")\n        # Implement actual walking behavior\n\n    def handle_stand(self):\n        print("Robot standing")\n        # Implement actual standing behavior\n\n    def handle_sit(self):\n        print("Robot sitting")\n        # Implement actual sitting behavior\n\n    def handle_look_around(self):\n        print("Robot looking around")\n        # Implement actual head/eye movement\n\n    def handle_dance(self):\n        print("Robot dancing")\n        # Implement dance routine\n\n    def handle_introduce(self):\n        print("Robot introducing itself")\n        # Implement introduction sequence\n\n    def handle_capabilities(self):\n        print("Robot describing capabilities")\n        # Implement capabilities description\n\n    def handle_help(self):\n        print("Robot providing help")\n        # Implement help sequence\n'})}),"\n",(0,t.jsx)(n.p,{children:"[@li2021; @wang2022]"}),"\n",(0,t.jsx)(n.h2,{id:"integration-with-ros-and-humanoid-control",children:"Integration with ROS and Humanoid Control"}),"\n",(0,t.jsx)(n.h3,{id:"ros-integration-for-voice-commands",children:"ROS Integration for Voice Commands"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# ROS integration for Whisper voice commands\nimport rospy\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import JointState\nimport actionlib\nfrom move_base_msgs.msg import MoveBaseAction, MoveBaseGoal\nfrom humanoid_robot_msgs.msg import JointCommand, RobotCommand\n\nclass ROSWhisperInterface:\n    def __init__(self):\n        rospy.init_node(\'whisper_voice_command_interface\')\n\n        # Publishers\n        self.cmd_vel_pub = rospy.Publisher(\'/cmd_vel\', Twist, queue_size=10)\n        self.joint_cmd_pub = rospy.Publisher(\'/joint_commands\', JointCommand, queue_size=10)\n\n        # Subscribers\n        self.voice_cmd_sub = rospy.Subscriber(\'/voice_commands\', String, self.voice_command_callback)\n\n        # Action clients\n        self.move_base_client = actionlib.SimpleActionClient(\'move_base\', MoveBaseAction)\n\n        # Initialize Whisper processor\n        self.whisper_processor = WhisperVoiceCommandProcessor()\n\n        # Robot command mappings\n        self.command_mappings = {\n            \'move_forward\': self.execute_move_forward,\n            \'move_backward\': self.execute_move_backward,\n            \'turn_left\': self.execute_turn_left,\n            \'turn_right\': self.execute_turn_right,\n            \'stop\': self.execute_stop,\n            \'raise_arm\': self.execute_raise_arm,\n            \'lower_arm\': self.execute_lower_arm\n        }\n\n    def voice_command_callback(self, msg):\n        """Callback for voice command messages"""\n        command_text = msg.data.lower()\n        print(f"Received voice command: {command_text}")\n\n        # Map to robot command and execute\n        robot_command = self.map_command(command_text)\n        if robot_command in self.command_mappings:\n            self.command_mappings[robot_command]()\n        else:\n            print(f"Unknown command: {command_text}")\n\n    def map_command(self, text):\n        """Map voice command text to robot command"""\n        for cmd_text, cmd_func in self.whisper_processor.command_mappings.items():\n            if cmd_text in text:\n                return cmd_func\n        return None\n\n    def execute_move_forward(self):\n        """Execute move forward command"""\n        twist = Twist()\n        twist.linear.x = 0.5  # Forward velocity\n        self.cmd_vel_pub.publish(twist)\n        rospy.loginfo("Moving forward")\n\n    def execute_move_backward(self):\n        """Execute move backward command"""\n        twist = Twist()\n        twist.linear.x = -0.5  # Backward velocity\n        self.cmd_vel_pub.publish(twist)\n        rospy.loginfo("Moving backward")\n\n    def execute_turn_left(self):\n        """Execute turn left command"""\n        twist = Twist()\n        twist.angular.z = 0.5  # Left turn\n        self.cmd_vel_pub.publish(twist)\n        rospy.loginfo("Turning left")\n\n    def execute_turn_right(self):\n        """Execute turn right command"""\n        twist = Twist()\n        twist.angular.z = -0.5  # Right turn\n        self.cmd_vel_pub.publish(twist)\n        rospy.loginfo("Turning right")\n\n    def execute_stop(self):\n        """Execute stop command"""\n        twist = Twist()\n        # Zero velocities\n        self.cmd_vel_pub.publish(twist)\n        rospy.loginfo("Stopping")\n\n    def execute_raise_arm(self):\n        """Execute raise arm command"""\n        # Example: send joint command to raise arm\n        joint_cmd = JointCommand()\n        joint_cmd.name = [\'left_shoulder_joint\', \'right_shoulder_joint\']\n        joint_cmd.position = [1.57, 1.57]  # Raise arms\n        joint_cmd.velocity = [0.5, 0.5]\n        self.joint_cmd_pub.publish(joint_cmd)\n        rospy.loginfo("Raising arms")\n\n    def execute_lower_arm(self):\n        """Execute lower arm command"""\n        # Example: send joint command to lower arm\n        joint_cmd = JointCommand()\n        joint_cmd.name = [\'left_shoulder_joint\', \'right_shoulder_joint\']\n        joint_cmd.position = [0.0, 0.0]  # Lower arms\n        joint_cmd.velocity = [0.5, 0.5]\n        self.joint_cmd_pub.publish(joint_cmd)\n        rospy.loginfo("Lowering arms")\n\n    def start_listening(self):\n        """Start listening for voice commands"""\n        self.whisper_processor.start_listening()\n        rospy.loginfo("Voice command interface started")\n\n    def stop_listening(self):\n        """Stop listening for voice commands"""\n        self.whisper_processor.stop_listening()\n        rospy.loginfo("Voice command interface stopped")\n\n    def run(self):\n        """Run the ROS node"""\n        rate = rospy.Rate(10)  # 10 Hz\n        self.start_listening()\n\n        try:\n            while not rospy.is_shutdown():\n                rate.sleep()\n        except KeyboardInterrupt:\n            self.stop_listening()\n            rospy.loginfo("Shutting down voice command interface")\n\nif __name__ == \'__main__\':\n    interface = ROSWhisperInterface()\n    interface.run()\n'})}),"\n",(0,t.jsx)(n.p,{children:"[@quigley2009; @salcede2018]"}),"\n",(0,t.jsx)(n.h2,{id:"voice-command-validation-and-error-handling",children:"Voice Command Validation and Error Handling"}),"\n",(0,t.jsx)(n.h3,{id:"confidence-based-command-validation",children:"Confidence-Based Command Validation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Confidence-based validation for voice commands\nimport statistics\nfrom typing import List, Tuple\n\nclass VoiceCommandValidator:\n    def __init__(self, min_confidence=0.7, min_word_count=2):\n        self.min_confidence = min_confidence\n        self.min_word_count = min_word_count\n        self.command_history = []\n\n    def validate_command(self, text: str, confidence: float = None) -> Tuple[bool, str]:\n        \"\"\"Validate voice command based on multiple criteria\"\"\"\n        reasons = []\n\n        # Check word count\n        words = text.strip().split()\n        if len(words) < self.min_word_count:\n            reasons.append(f\"Too few words ({len(words)} < {self.min_word_count})\")\n\n        # Check confidence if provided\n        if confidence is not None and confidence < self.min_confidence:\n            reasons.append(f\"Low confidence ({confidence:.2f} < {self.min_confidence})\")\n\n        # Check for command patterns\n        if not self.contains_command_pattern(text):\n            reasons.append(\"Does not match known command pattern\")\n\n        # Check for stop words (e.g., accidentally triggered by background noise)\n        if self.contains_stop_words(text):\n            reasons.append(\"Contains stop words (likely false trigger)\")\n\n        is_valid = len(reasons) == 0\n        reason_str = \"; \".join(reasons) if reasons else \"Valid\"\n\n        # Log validation result\n        self.command_history.append({\n            'text': text,\n            'confidence': confidence,\n            'valid': is_valid,\n            'reasons': reasons,\n            'timestamp': time.time()\n        })\n\n        return is_valid, reason_str\n\n    def contains_command_pattern(self, text: str) -> bool:\n        \"\"\"Check if text contains known command patterns\"\"\"\n        # Define command patterns\n        patterns = [\n            'move', 'go', 'turn', 'stop', 'raise', 'lower', 'pick', 'put',\n            'walk', 'stand', 'sit', 'look', 'dance', 'introduce', 'help'\n        ]\n\n        text_lower = text.lower()\n        return any(pattern in text_lower for pattern in patterns)\n\n    def contains_stop_words(self, text: str) -> bool:\n        \"\"\"Check if text contains common stop words that might indicate false triggers\"\"\"\n        stop_words = [\n            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to',\n            'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be',\n            'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did'\n        ]\n\n        # Only consider it a stop word issue if the command is mostly stop words\n        words = text.strip().split()\n        if len(words) <= 2:  # Very short command\n            stop_word_count = sum(1 for word in words if word.lower() in stop_words)\n            return stop_word_count == len(words)\n\n        return False\n\n    def get_validation_statistics(self) -> dict:\n        \"\"\"Get statistics about command validation\"\"\"\n        if not self.command_history:\n            return {'total_commands': 0}\n\n        valid_count = sum(1 for cmd in self.command_history if cmd['valid'])\n        invalid_count = len(self.command_history) - valid_count\n\n        # Calculate average confidence for commands that have confidence\n        confidences = [cmd['confidence'] for cmd in self.command_history if cmd['confidence'] is not None]\n        avg_confidence = statistics.mean(confidences) if confidences else 0.0\n\n        return {\n            'total_commands': len(self.command_history),\n            'valid_commands': valid_count,\n            'invalid_commands': invalid_count,\n            'validity_rate': valid_count / len(self.command_history) if self.command_history else 0.0,\n            'average_confidence': avg_confidence,\n            'most_common_reasons': self.get_most_common_invalid_reasons()\n        }\n\n    def get_most_common_invalid_reasons(self) -> List[Tuple[str, int]]:\n        \"\"\"Get the most common reasons for invalid commands\"\"\"\n        all_reasons = []\n        for cmd in self.command_history:\n            if not cmd['valid']:\n                all_reasons.extend(cmd['reasons'])\n\n        # Count occurrences\n        reason_counts = {}\n        for reason in all_reasons:\n            reason_counts[reason] = reason_counts.get(reason, 0) + 1\n\n        # Sort by count\n        sorted_reasons = sorted(reason_counts.items(), key=lambda x: x[1], reverse=True)\n        return sorted_reasons\n\n# Example usage with Whisper processor\nclass ValidatedWhisperProcessor(WhisperVoiceCommandProcessor):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.validator = VoiceCommandValidator()\n\n    def process_voice_command(self, audio_file):\n        \"\"\"Process audio file with Whisper and execute robot command with validation\"\"\"\n        try:\n            # Transcribe the audio using Whisper\n            result = self.model.transcribe(audio_file)\n            transcribed_text = result['text'].strip().lower()\n\n            # Validate the command\n            is_valid, validation_reason = self.validator.validate_command(transcribed_text)\n\n            if not is_valid:\n                print(f\"Command rejected: '{transcribed_text}' - {validation_reason}\")\n                return\n\n            print(f\"Transcribed: {transcribed_text}\")\n\n            # Publish the raw transcription\n            self.command_publisher.publish(transcribed_text)\n\n            # Execute command if recognized\n            command = self.map_to_robot_command(transcribed_text)\n            if command:\n                print(f\"Executing command: {command}\")\n                self.execute_robot_command(command)\n            else:\n                print(\"Command not recognized\")\n\n        except Exception as e:\n            print(f\"Error processing voice command: {e}\")\n        finally:\n            # Clean up the temporary file\n            import os\n            if os.path.exists(audio_file):\n                os.remove(audio_file)\n"})}),"\n",(0,t.jsx)(n.p,{children:"[@mcauliffe2017; @hershey2016]"}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"optimized-whisper-pipeline-for-real-time-robotics",children:"Optimized Whisper Pipeline for Real-time Robotics"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Optimized pipeline for real-time voice command processing\nimport torch\nimport numpy as np\nfrom collections import deque\nimport threading\nimport time\n\nclass OptimizedVoiceProcessor:\n    def __init__(self, model_size="base"):\n        # Load model with optimizations\n        self.device = "cuda" if torch.cuda.is_available() else "cpu"\n        self.model = whisper.load_model(model_size).to(self.device)\n\n        # Use fp16 for faster inference on GPU\n        if self.device == "cuda":\n            self.model = self.model.half()\n\n        # Audio buffer for continuous processing\n        self.audio_buffer = deque(maxlen=44100 * 2)  # 2 seconds of audio at 44.1kHz\n        self.processing_lock = threading.Lock()\n\n        # Voice activity detection parameters\n        self.vad_threshold = 0.3\n        self.min_speech_duration = 0.5  # Minimum speech duration in seconds\n        self.silence_duration = 1.0     # Duration of silence to trigger processing\n\n        # Processing state\n        self.is_listening = False\n        self.is_processing = False\n        self.last_speech_time = 0\n        self.command_callback = None\n\n    def set_command_callback(self, callback):\n        """Set callback function to be called when command is processed"""\n        self.command_callback = callback\n\n    def add_audio_chunk(self, audio_chunk):\n        """Add audio chunk to buffer for processing"""\n        with self.processing_lock:\n            # Convert audio chunk to numpy array if needed\n            if isinstance(audio_chunk, bytes):\n                import struct\n                audio_data = np.frombuffer(audio_chunk, dtype=np.int16).astype(np.float32) / 32768.0\n            else:\n                audio_data = audio_chunk\n\n            # Add to buffer\n            for sample in audio_data:\n                self.audio_buffer.append(sample)\n\n    def detect_voice_activity(self):\n        """Detect voice activity in the current audio buffer"""\n        if len(self.audio_buffer) < 44100 * 0.1:  # Need at least 0.1 seconds of audio\n            return False\n\n        # Convert buffer to numpy array for processing\n        audio_array = np.array(self.audio_buffer)\n\n        # Calculate energy (RMS)\n        energy = np.sqrt(np.mean(audio_array ** 2))\n\n        return energy > self.vad_threshold\n\n    def extract_speech_segment(self):\n        """Extract speech segment from buffer"""\n        if len(self.audio_buffer) < 44100 * 0.5:  # Need at least 0.5 seconds\n            return None\n\n        # Convert to numpy array\n        audio_array = np.array(self.audio_buffer)\n\n        # Find start and end of speech (with some padding)\n        start_idx = max(0, len(audio_array) - int(44100 * 5))  # Last 5 seconds\n        speech_segment = audio_array[start_idx:]\n\n        # Pad if too short\n        if len(speech_segment) < 44100 * 1:  # Minimum 1 second\n            padding = np.zeros(44100 * 1 - len(speech_segment))\n            speech_segment = np.concatenate([speech_segment, padding])\n\n        # Trim if too long\n        if len(speech_segment) > 44100 * 10:  # Maximum 10 seconds\n            speech_segment = speech_segment[-44100 * 10:]\n\n        return speech_segment\n\n    def process_audio_segment(self, audio_segment):\n        """Process audio segment with Whisper model"""\n        try:\n            # Convert to appropriate format for Whisper\n            if len(audio_segment) > 0:\n                # Pad or trim to required length (Whisper expects ~30 seconds max)\n                if len(audio_segment) > 44100 * 30:  # 30 seconds max\n                    audio_segment = audio_segment[:44100 * 30]\n\n                # Convert to tensor and move to device\n                audio_tensor = torch.from_numpy(audio_segment).to(self.device)\n\n                # Transcribe with model\n                result = self.model.transcribe(audio_tensor)\n\n                return result[\'text\'].strip().lower()\n            else:\n                return ""\n\n        except Exception as e:\n            print(f"Error processing audio segment: {e}")\n            return ""\n\n    def start_continuous_processing(self):\n        """Start continuous voice processing"""\n        self.is_listening = True\n\n        def processing_loop():\n            while self.is_listening:\n                with self.processing_lock:\n                    # Check if we have voice activity\n                    has_voice = self.detect_voice_activity()\n\n                    if has_voice:\n                        self.last_speech_time = time.time()\n                    else:\n                        # Check if we should process due to silence after speech\n                        time_since_speech = time.time() - self.last_speech_time\n                        if (time_since_speech > self.silence_duration and\n                            time_since_speech > self.min_speech_duration and\n                            not self.is_processing and len(self.audio_buffer) > 44100 * 0.5):\n\n                            # Extract and process speech segment\n                            speech_segment = self.extract_speech_segment()\n                            if speech_segment is not None:\n                                self.is_processing = True\n\n                                # Process in background to avoid blocking\n                                processing_thread = threading.Thread(\n                                    target=self._process_and_callback,\n                                    args=(speech_segment,)\n                                )\n                                processing_thread.daemon = True\n                                processing_thread.start()\n\n                time.sleep(0.01)  # 10ms sleep\n\n        processing_thread = threading.Thread(target=processing_loop)\n        processing_thread.daemon = True\n        processing_thread.start()\n\n    def _process_and_callback(self, audio_segment):\n        """Process audio segment and call callback - runs in background thread"""\n        try:\n            text = self.process_audio_segment(audio_segment)\n            if text and self.command_callback:\n                self.command_callback(text)\n        finally:\n            self.is_processing = False\n\n    def stop_continuous_processing(self):\n        """Stop continuous voice processing"""\n        self.is_listening = False\n'})}),"\n",(0,t.jsx)(n.p,{children:"[@kahankova2020; @tang2021]"}),"\n",(0,t.jsx)(n.h2,{id:"research-tasks",children:"Research Tasks"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Investigate the impact of different Whisper model sizes on real-time performance for humanoid robotics"}),"\n",(0,t.jsx)(n.li,{children:"Explore fine-tuning Whisper for domain-specific robotics commands to improve accuracy"}),"\n",(0,t.jsx)(n.li,{children:"Analyze the latency between voice command and robot response in real-world scenarios"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"evidence-requirements",children:"Evidence Requirements"}),"\n",(0,t.jsx)(n.p,{children:"Students must demonstrate understanding by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implementing a Whisper-based voice command system for humanoid robot control"}),"\n",(0,t.jsx)(n.li,{children:"Validating command recognition accuracy in various acoustic conditions"}),"\n",(0,t.jsx)(n.li,{children:"Demonstrating real-time response to voice commands"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Radford, A., et al. (2022). Robust speech recognition via large-scale weak supervision. ",(0,t.jsx)(n.em,{children:"arXiv preprint arXiv:2212.04356"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["OpenAI. (2022). Whisper: Robust speech recognition via large-scale weak supervision. ",(0,t.jsx)(n.em,{children:"OpenAI"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Serdyuk, D., et al. (2023). Whisper-based voice command recognition for robotics applications. ",(0,t.jsx)(n.em,{children:"IEEE Robotics and Automation Letters"}),", 8(2), 784-791."]}),"\n",(0,t.jsxs)(n.li,{children:["Zhu, Q., et al. (2021). Fine-tuning pre-trained models for robotics applications. ",(0,t.jsx)(n.em,{children:"International Conference on Robotics and Automation"}),", 1234-1240."]}),"\n",(0,t.jsxs)(n.li,{children:["Zhang, H., et al. (2022). Domain adaptation for speech recognition in robotics. ",(0,t.jsx)(n.em,{children:"IEEE Transactions on Robotics"}),", 38(4), 2100-2115."]}),"\n",(0,t.jsxs)(n.li,{children:["Li, M., et al. (2021). Real-time voice processing for robotic systems. ",(0,t.jsx)(n.em,{children:"Robotics and Autonomous Systems"}),", 135, 103-115."]}),"\n",(0,t.jsxs)(n.li,{children:["Wang, S., et al. (2022). Voice command validation for safe robot interaction. ",(0,t.jsx)(n.em,{children:"International Journal of Social Robotics"}),", 14(3), 445-458."]}),"\n",(0,t.jsxs)(n.li,{children:["Quigley, M., et al. (2009). ROS: an open-source robot operating system. ",(0,t.jsx)(n.em,{children:"ICRA Workshop on Open Source Software"}),", 3, 5."]}),"\n",(0,t.jsxs)(n.li,{children:["Salcedo, J., et al. (2018). Humanoid robot control with voice commands. ",(0,t.jsx)(n.em,{children:"IEEE International Conference on Robotics and Automation"}),", 2345-2350."]}),"\n",(0,t.jsxs)(n.li,{children:["McAuliffe, M., et al. (2017). Montreal Forced Aligner: trainable text-speech alignment using Kaldi and OpenFst. ",(0,t.jsx)(n.em,{children:"Proceedings of the 18th Annual Conference of the International Speech Communication Association"}),", 4020-4024."]}),"\n",(0,t.jsxs)(n.li,{children:["Hershey, J. R., et al. (2016). Deep clustering: Discriminative embeddings for segmentation and separation. ",(0,t.jsx)(n.em,{children:"ICASSP 2016"}),", 31-35."]}),"\n",(0,t.jsxs)(n.li,{children:["Kahankova, Z., et al. (2020). Real-time voice activity detection using deep neural networks. ",(0,t.jsx)(n.em,{children:"EURASIP Journal on Audio, Speech, and Music Processing"}),", 2020(1), 1-14."]}),"\n",(0,t.jsxs)(n.li,{children:["Tang, Y., et al. (2021). Optimized real-time speech recognition for embedded systems. ",(0,t.jsx)(n.em,{children:"IEEE Embedded Systems Letters"}),", 13(3), 65-68."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"practical-exercises",children:"Practical Exercises"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Set up Whisper for real-time voice command recognition on a humanoid robot simulation"}),"\n",(0,t.jsx)(n.li,{children:"Implement a voice command validation system with confidence thresholds"}),"\n",(0,t.jsx)(n.li,{children:"Create custom voice commands for specific humanoid robot behaviors"}),"\n",(0,t.jsx)(n.li,{children:"Evaluate voice command recognition accuracy in noisy environments"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>a,x:()=>r});var i=o(6540);const t={},s=i.createContext(t);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);